# -*- coding: utf-8 -*-
"""#  K3 – Ürün Özellik & Kategori (NLP + Öneri Sistemi)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_dFkka32t4Ck56yGCu39cml50CpFJNcY

#  K3 – Ürün Özellik & Kategori (NLP + Öneri Sistemi)

1. Kütüphanelerin ve Kaynakların Yüklenmesi
"""

import pandas as pd
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import seaborn as sns
from collections import Counter
import gzip
import shutil

nltk.download('stopwords')
stop_words = set(ENGLISH_STOP_WORDS)

"""2. Google Drive Bağlantısı

"""

from google.colab import drive
drive.mount('/content/drive')

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)

"""3. .gz Dosyasını Açma ve CSV’ye Çevirme

*   Veri seti Open Food Facts projesine ait ürün bilgilerini içeriyor.
*   İlk adımda .gz formatındaki sıkıştırılmış dosya açılarak .csv formatına dönüştürüldü.
"""

# --- .gz Dosyasını Aç ve CSV Olarak Kaydet ---
gz_path = "/content/drive/MyDrive/en.openfoodfacts.org.products.csv.gz"
csv_path = "/content/drive/MyDrive/products.csv"
with gzip.open(gz_path, 'rb') as f_in:
    with open(csv_path, 'wb') as f_out:
        shutil.copyfileobj(f_in, f_out)

"""4. CSV’yi Parça Parça Okuma (Chunking)

*   Veri seti çok büyük olduğu için chunksize=100000 parametresi ile parça parça okundu.
*   Yalnızca gerekli sütunlar alındı:
*   product_name
*   categories
*   ingredients_text
*   purchase_places
*   Eksik veriler temizlenerek tüm parçalar birleştirildi.
"""

# --- CSV'yi Parça Parça Oku, Gerekli Sütunlar ---
chunks = pd.read_csv(
    csv_path, sep='\t',
    usecols=["product_name", "categories", "ingredients_text", "purchase_places"],
    chunksize=100000, low_memory=False
)

# Eksik verileri at ve chunkları listeye ekle
df_list = []
for chunk in chunks:
    chunk = chunk.dropna(subset=["product_name", "categories", "ingredients_text"])
    df_list.append(chunk)

# --- Tüm chunkları birleştir ---
df_food = pd.concat(df_list, ignore_index=True)

"""5. New York Ürünlerinin Filtrelenmesi
*   purchase_places sütununda "New York" geçen ürünler seçildi ve df_ny adında yeni bir DataFrame oluşturuldu.
*   Bu adım, analizin belirli bir bölgeye (New York) odaklanmasını sağladı.
"""

# Sadece New York'ta satılan ürünler
df_ny = df_food[df_food["purchase_places"].str.contains("new york", case=False, na=False)].reset_index(drop=True)

"""6. Metin Temizleme Fonksiyonu

*   Metinleri standartlaştırmak için özel bir clean_text fonksiyonu yazıldı:
*   Metinler küçük harfe çevrildi.
*   Sayılar ve noktalama işaretleri kaldırıldı.
*   Stopword’ler (önemsiz kelimeler) çıkarıldı.
"""

# --- Temizleme Fonksiyonu ---
def clean_text(text):
    if pd.isna(text):
        return ""
    text = str(text).lower()  # Küçük harfe çevir
    text = re.sub(r"[^a-z\s]", "", text)  # Noktalama ve sayı çıkar
    words = text.split()
    words = [w for w in words if w not in stop_words]  # Stopword çıkar
    return " ".join(words)  # Temizlenmiş metin

"""7. full_text Sütununun Oluşturulması

*   Analiz kolaylığı için, ürünün adı, kategorileri, içerikleri ve satış noktalarını birleştiren full_text sütunu oluşturuldu.
*   Ayrıca bu metin clean_text fonksiyonundan geçirilerek full_text_clean sütunu üretildi.


"""

# --- full_text Oluştur ve Temizle (purchase_places dahil) ---
df_ny["full_text"] = (
    df_ny["product_name"].astype(str) + " " +
    df_ny["categories"].astype(str) + " " +
    df_ny["ingredients_text"].astype(str) + " " +
    df_ny["purchase_places"].astype(str))

df_ny["full_text_clean"] = df_ny["full_text"].apply(clean_text)

"""8. İçerik Anahtar Kelimelerinin Çıkarılması (Binary Özellikler)

*   Belirli içeriklerin varlığı binary (0/1) olarak işaretlendi:
*   has_sugar
*   has_lactose
*   has_gluten
*   is_vegan
*   is_vegetarian












"""

# --- İçeriğe sahip mi değil mi (binary sütunlar) ---
keywords = {
    "has_sugar": "sugar",
    "has_lactose": "lactose",
    "has_gluten": "gluten",
    "is_vegan": "vegan",
    "is_vegetarian": "vegetarian"
}

for col_name, keyword in keywords.items():
    df_ny[col_name] = df_ny["ingredients_text"].str.lower().str.contains(keyword, na=False).astype(int)

"""9. İçerik Dağılımlarının Hesaplanması

*   Bu özelliklerin (ör. gluten var mı?) veri setinde kaç üründe bulunduğu sayısal olarak analiz edildi.
*   Bu sayede veri setinde hangi içeriklerin baskın olduğu görülebilir.
"""

# --- İçeriklerin sayı bazında dağılımı ---
for col_name in keywords.keys():
    print(f"{col_name} içerik dağılımı:")
    print(df_ny[col_name].value_counts())
    print("\n")

"""10. BERT Modeli ile Embedding


*   Bu adımda SentenceTransformer kütüphanesinden "paraphrase-MiniLM-L6-v2" adlı önceden eğitilmiş BERT tabanlı model kullanılarak her ürünün açıklamasının (full_text_clean) vektör temsilleri (embedding) oluşturuluyor.
*   Amaç: Metinleri sayısal vektörlere çevirerek, ürünler arasındaki anlamsal benzerliği hesaplamak.
*   model.encode() ile tüm metinler için embedding üretiliyor.


"""

# --- BERT Modeli ile Embedding ---
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
print("Embedding oluşturuluyor...")
embeddings = model.encode(df_ny["full_text_clean"].tolist(), show_progress_bar=True)

"""11. Cosine Similarity Matrisi

*   Oluşturulan BERT embedding'leri kullanarak ürünler arasındaki kosinüs benzerliği hesaplanıyor.
*   Cosine Similarity 1'e ne kadar yakınsa, iki ürün o kadar benzer.
*   Sonuç: similarity_matrix değişkeninde tüm ürünler arası benzerlik skorları saklanıyor.
"""

# --- Cosine Similarity Matrisi ---
print("Benzerlik matrisi hesaplanıyor...")
similarity_matrix = cosine_similarity(embeddings)

"""12. Öneri ve Grafik Fonksiyonu

*  öneri_ve_grafik() fonksiyonu:
*   Seçilen bir ürünün (indeks numarası ile) en benzer ürünlerini bulur.
*   Bu ürünleri benzerlik skorları ile birlikte ekrana yazar.
*   İsteğe bağlı olarak (show_plot=True) skorların bar grafiğini çizer.

*  Örnek çıktı:
*   Örnek ürün adı
*   Benzer ürün listesi ve skorları
*   Cosine benzerlik grafiği


"""

# --- Öneri ve Grafik Fonksiyonu ---
def öneri_ve_grafik(indeks, sim_matrix, ürün_sayısı=5, show_plot=False):
    sim_scores = sim_matrix[indeks].copy()
    sim_scores[indeks] = 0  # Kendini çıkar
    top_indices = np.argsort(sim_scores)[::-1][:ürün_sayısı]
    öneriler = df_ny.iloc[top_indices][["product_name", "categories"]].copy()
    skorlar = sim_scores[top_indices]

    print(f"Örnek ürün: {df_ny.iloc[indeks]['product_name']}\n")
    print("Önerilen ürünler ve benzerlik skorları:")
    for i, (isim, skor) in enumerate(zip(öneriler["product_name"], skorlar)):
        print(f"{i+1}. {isim} (Benzerlik: {skor:.4f})")

    if show_plot:
        plt.figure(figsize=(10,6))
        plt.barh(range(ürün_sayısı), skorlar, color='skyblue')
        plt.yticks(range(ürün_sayısı), öneriler["product_name"])
        plt.gca().invert_yaxis()
        plt.xlabel("Cosine Benzerlik Skoru")
        plt.title(f"\"{df_ny.iloc[indeks]['product_name']}\" için Top {ürün_sayısı} Öneri")
        plt.show()

öneri_ve_grafik(0, similarity_matrix, ürün_sayısı=10, show_plot=True)

"""13. TF-IDF Alternatifi

*  BERT yerine TF-IDF (Term Frequency - Inverse Document Frequency) yaklaşımı ile metin vektörleri oluşturulur.
* TfidfVectorizer(max_features=1000) ile en fazla 1000 kelime seçilir.
* cosine_similarity ile ürünler arası TF-IDF benzerliği hesaplanır.
* tfidf_grafik() fonksiyonu belirli bir ürün için en baskın 10 kelimeyi görselleştirir.
* Amaç: Daha basit bir model ile kelime bazlı ürün karşılaştırması yapmak.


"""

# --- TF-IDF Alternatifi ---
tfidf = TfidfVectorizer(max_features=1000)
tfidf_matrix = tfidf.fit_transform(df_ny["full_text_clean"])
tfidf_similarity = cosine_similarity(tfidf_matrix)
dense_matrix = tfidf_matrix.toarray()
feature_names = tfidf.get_feature_names_out()

print(feature_names[:20])  # İlk 20 kelime

def tfidf_grafik(indeks):
    row = dense_matrix[indeks]  # numpy array → [n_features]
    top_indices = np.argsort(row)[::-1][:10]
    top_words = [feature_names[i] for i in top_indices]
    top_scores = row[top_indices]

    ürün_ismi = df_ny.iloc[indeks]["product_name"]

    plt.figure(figsize=(10, 6))
    plt.barh(range(10), top_scores, color='orange')
    plt.yticks(range(10), top_words)
    plt.gca().invert_yaxis()
    plt.xlabel("TF-IDF Skoru")
    plt.title(f"\"{ürün_ismi}\" ürününde en baskın 10 kelime")
    plt.tight_layout()
    plt.show()

tfidf_grafik(0)
tfidf_grafik(380)
tfidf_grafik(50)

"""14. Apriori ile Cross-Sell Kuralları

* Burada ürün kategorilerinden birliktelik kuralları çıkarılır.
* TransactionEncoder ile kategoriler makineye uygun hale getirilir.
* apriori() algoritması ile en sık birlikte görülen kategori kombinasyonları bulunur.
* association_rules() ile support, confidence, lift metrikleri hesaplanır.
* Lift değeri yüksek olan kurallar, çapraz satış (cross-sell) önerileri için daha anlamlıdır.
* Sonuç olarak en yüksek lift değerine sahip ilk 10 kural ekrana yazdırılır.
"""

# --- Apriori ile Cross-Sell Kuralları ---
category_lists = df_ny['categories'].str.lower().str.split(',')
te = TransactionEncoder()
te_ary = te.fit(category_lists).transform(category_lists)
df_tf = pd.DataFrame(te_ary, columns=te.columns_)
frequent_itemsets = apriori(df_tf, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

# En yüksek Lift değerine sahip 10 ürün
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values(by='lift', ascending=False).head(10))

"""15. Hibrit Öneri Sistemi

* Bu fonksiyon, TF-IDF ve BERT tabanlı benzerlik skorlarını birleştirerek daha dengeli öneriler sunar.
* TF-IDF → İstatistiksel olarak kelime önemini ölçer.
* BERT → Anlamsal (semantic) benzerliği yakalar.

  Nasıl çalışır?
* Seçilen ürünün TF-IDF benzerlik skoru alınır.
* Aynı ürün için BERT benzerlik skoru alınır.
* İki skorun ortalaması hesaplanır.
* En yüksek skora sahip ürünler öneri olarak listelenir.
"""

# --- Hibrit Öneri Fonksiyonu ---
def hibrit_öner(df, index, tfidf_sim, bert_sim, rules, top_n=5):
    tfidf_scores = tfidf_sim[index].copy()
    bert_scores = bert_sim[index].copy()
    combined = (tfidf_scores + bert_scores) / 2
    combined[index] = 0
    top_indices = np.argsort(combined)[::-1][:top_n]
    öneriler = df.iloc[top_indices]["product_name"].tolist()
    print(f"\n\033[1mHibrit öneri sonucu (TF-IDF + BERT)\033[0m")
    for i, ürün in enumerate(öneriler):
        print(f"{i+1}. {ürün} (skor: {combined[top_indices[i]]:.4f})")

# Örnek hibrit öneri
öneri_ve_grafik(87, similarity_matrix, ürün_sayısı=10)
hibrit_öner(df_ny, 0, tfidf_similarity, similarity_matrix, rules)

"""16. Similarity Score Fonksiyonu

* Bu fonksiyon, belirli bir ürün için en benzer ürünleri ve skorlarını döndürür.
* Kullanıcıya hem ürün ismini hem de benzerlik oranını verir.
Öneri listesi hazırlanırken arka planda çalışabilir.


"""

# --- Similarity Score Fonksiyonu ---
def similarity_score(indeks, sim_matrix, df, n=5):
    """
    Verilen ürün indeksine göre en benzer n ürünü ve similarity skorlarını döner.

    Args:
        indeks (int): Öneri yapılacak ürünün indeksi
        sim_matrix (numpy.ndarray): Ürünler arası similarity matrisi
        df (pandas.DataFrame): Ürün bilgilerini içeren DataFrame (en az 'product_name' olmalı)
        n (int): Kaç ürün önerileceği

    Returns:
        List of tuples: [(ürün_ismi, similarity_skoru), ...]
    """
    sim_scores = sim_matrix[indeks].copy()
    sim_scores[indeks] = 0  # Kendini çıkar
    top_indices = sim_scores.argsort()[::-1][:n]

    öneriler = []
    for i in top_indices:
        isim = df.iloc[i]["product_name"]
        skor = sim_scores[i]
        öneriler.append((isim, skor))
    return öneriler

# Örnek kullanım
indeks = 0  # İlk ürün için
benzer_urunler = similarity_score(indeks, similarity_matrix, df_ny, n=5)
print(benzer_urunler)

"""17. Hibrit Vektörlerin Oluşturulması

* BERT embedding’leri ile TF-IDF vektörleri birleştirilerek (concatenate) yeni bir hibrit vektör oluşturulur.
* Daha sonra StandardScaler ile tüm özellikler aynı ölçeğe getirilir.
* Bu hibrit vektörler, kümeleme algoritmaları için giriş verisi olarak kullanılır.
"""

from sklearn.preprocessing import StandardScaler

# --- BERT embeddingleri oluştur ---
model = SentenceTransformer("paraphrase-MiniLM-L6-v2")
embeddings = model.encode(df_ny["full_text_clean"].tolist(), show_progress_bar=True)

# --- TF-IDF matrisi oluştur ---
tfidf = TfidfVectorizer(max_features=1000)
tfidf_matrix = tfidf.fit_transform(df_ny["full_text_clean"])
dense_matrix = tfidf_matrix.toarray()

# embeddings: BERT embedding, numpy array (n_samples, bert_dim)
# tfidf_matrix: TF-IDF sparse matrix, (n_samples, n_features)

# TF-IDF dense
tfidf_dense = tfidf_matrix.toarray()

# Hibrit vektörler (concatenate)
hybrid_vectors = np.concatenate([embeddings, tfidf_dense], axis=1)

# Ölçeklendirme
scaler = StandardScaler()
hybrid_scaled = scaler.fit_transform(hybrid_vectors)

# hybrid_scaled artık kümeleme için hazır ölçeklendirilmiş embedding vektörüdür

"""18. Beş Farklı Kümeleme Modeli ile Optimizasyon
* Fonksiyon, KMeans, MiniBatchKMeans, Agglomerative Clustering, GaussianMixture ve DBSCAN algoritmaları için en iyi parametreleri bulur.
* Silhouette Score metriği kullanılarak en iyi küme sayısı veya parametreler seçilir.
* Böylece, veriye en uygun kümeleme modeli belirlenir.
"""

from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

def en_iyi_parametre_5model(X):
    results = {}

    # 1. KMeans, MiniBatchKMeans, Agglomerative, GaussianMixture için k taraması
    cluster_algorithms = {
        "KMeans": KMeans,
        "MiniBatchKMeans": MiniBatchKMeans,
        "AgglomerativeClustering": AgglomerativeClustering,
        "GaussianMixture": GaussianMixture
    }

    for name, algorithm in cluster_algorithms.items():
        best_score = -1
        best_k = None
        for k in range(2, 11):
            try:
                if name == "GaussianMixture":
                    model = algorithm(n_components=k, random_state=42)
                    labels = model.fit_predict(X)
                else:
                    model = algorithm(n_clusters=k, random_state=42)
                    labels = model.fit_predict(X)

                if len(set(labels)) < 2:
                    continue

                score = silhouette_score(X, labels)
                if score > best_score:
                    best_score = score
                    best_k = k
            except:
                continue
        results[name] = {"best_k": best_k, "silhouette_score": best_score}

    # 2. DBSCAN için eps ve min_samples araması
    best_score = -1
    best_params = None
    eps_values = np.arange(0.3, 1.1, 0.1)
    min_samples_values = range(3, 10)

    for eps in eps_values:
        for min_s in min_samples_values:
            try:
                model = DBSCAN(eps=eps, min_samples=min_s)
                labels = model.fit_predict(X)
                unique_labels = set(labels)
                if len(unique_labels) < 2 or (len(unique_labels) == 2 and -1 in unique_labels):
                    continue
                score = silhouette_score(X, labels)
                if score > best_score:
                    best_score = score
                    best_params = {"eps": eps, "min_samples": min_s}
            except:
                continue
    results["DBSCAN"] = {"best_params": best_params, "silhouette_score": best_score}

    return results

hybrid_vectors = (tfidf_similarity +  similarity_matrix)/2
sonuçlar = en_iyi_parametre_5model(hybrid_vectors)
print(sonuçlar)

"""19. PCA ile Boyut İndirgeme ve Görselleştirme

*  Hibrit vektörler PCA ile 2 boyuta indirgenir.
* Hem KMeans hem de GaussianMixture ile kümeleme yapılır.
* Sonuçlar renkli scatter plot şeklinde görselleştirilir.
* Bu sayede kümelerin ayrışma durumu görsel olarak incelenebilir.

Peki Neden PCA?

  PCA (Principal Component Analysis) kullanma amacı:

* Boyut indirgeme: Yüksek boyutlu veriyi, en çok varyansı açıklayan daha az boyutlu (ör. 2D veya 3D) bir uzaya indirerek hesaplama maliyetini ve gürültüyü azaltmak.

* Görselleştirme: Yüzlerce/ binlerce özellik taşıyan veriyi iki boyutta gösterip kümelerin veya örüntülerin insan gözüyle daha kolay anlaşılmasını sağlamak.

* Özellik seçimi: En çok bilgi taşıyan bileşenleri ortaya çıkararak modelin performansını artırmak ve overfitting riskini azaltmak.
"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

# --- PCA ile 2 boyuta indir ---
pca = PCA(n_components=2, random_state=42)
X_2d = pca.fit_transform(hybrid_vectors)

# --- En iyi parametrelerle modeller ---
best_k = 4

# KMeans
kmeans = KMeans(n_clusters=best_k, random_state=42)
labels_kmeans = kmeans.fit_predict(hybrid_vectors)

# GaussianMixture
gmm = GaussianMixture(n_components=best_k, random_state=42)
labels_gmm = gmm.fit_predict(hybrid_vectors)

# --- Görselleştirme ---
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# KMeans görseli
sns.scatterplot(
    x=X_2d[:, 0], y=X_2d[:, 1],
    hue=labels_kmeans, palette='tab10', ax=axes[0]
)
axes[0].set_title("KMeans (k=4)")

# GaussianMixture görseli
sns.scatterplot(
    x=X_2d[:, 0], y=X_2d[:, 1],
    hue=labels_gmm, palette='tab10', ax=axes[1]
)
axes[1].set_title("GaussianMixture (k=4)")

plt.tight_layout()
plt.show()

"""20. Veri Setini Kaydetme

* to_csv() metodu kullanılarak Google Drive içine (/content/drive/MyDrive/) kaydedilir.
* index=False parametresi, satır numaralarının dosyaya yazılmamasını sağlar.
* Böylece veri tekrar yüklenip analiz edilebilir, model yeniden eğitilebilir veya başka projelerde kullanılabilir.
"""

# --- DataFrame Kaydet ve Oku ---
df_ny.to_csv("/content/drive/MyDrive/df_ny_final.csv", index=False)


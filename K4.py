# -*- coding: utf-8 -*-
"""K4_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VER_RKI-W9dci5P9KYJ3RPQUdwtG_YDy
"""

import os
outdir = "/content/outputs"
os.makedirs(outdir, exist_ok=True)
os.makedirs(f"{outdir}/eda", exist_ok=True)

!pip install osmnx geopandas rapidfuzz geopy xgboost shap tqdm hdbscan

import osmnx as ox
import pandas as pd

# 1. NYC yol ağını OpenStreetMap'ten çek
graph_nyc = ox.graph_from_place("New York City, New York, USA", network_type="drive")

# 2. Graph objesini GeoDataFrame'lere çevir
nodes, edges = ox.graph_to_gdfs(graph_nyc)

# 3. CSV olarak kaydet (ilk defa çekildiği için)
nodes.to_csv("nyc_nodes.csv", index=False)
edges.to_csv("nyc_edges.csv", index=False)

print(" Yol ağı verileri kaydedildi.")
print("Nodes:", nodes.shape)
print("Edges:", edges.shape)

# 4. Yol verisi sütunlarını yazdır
print("\nEdge Columns:")
print(edges.columns.tolist())

# 5. Yol tipi dağılımı için highway kolonunu temizle
edges['highway_str'] = edges['highway'].apply(lambda x: ', '.join(x) if isinstance(x, list) else str(x))
highway_dist = edges['highway_str'].value_counts().head(20)
print(" Top 20 Most Common Road Types:")
print(highway_dist)

# 6. Yol uzunluğu istatistikleri
print(" Road Length Statistics (meters):")
print(edges['length'].describe())

# 7. Yol tipine göre ortalama uzunluk (en uzun 10)
avg_length_by_type = edges.groupby('highway_str')['length'].mean().sort_values(ascending=False).head(10)
print(" Top 10 Road Types by Average Length:")
print(avg_length_by_type)

# HÜCRE 2 — food_order.csv temel kontroller (orijinal EDA'n)

import pandas as pd
import numpy as np

df_raw = pd.read_csv("/content/food_order.csv")

print(" Dataset Shape:", df_raw.shape)
print("Column Names:", df_raw.columns.tolist())
print(" First Few Rows:\n", df_raw.head())
print(" Missing Values (Before):\n", df_raw.isnull().sum())
print("Data Types:\n", df_raw.dtypes)

# Basit doldurma (orijinalindeki gibi)
for col in df_raw.select_dtypes(include=['float64', 'int64']).columns:
    df_raw[col] = df_raw[col].fillna(df_raw[col].mean())

for col in df_raw.select_dtypes(include=['object']).columns:
    df_raw[col] = df_raw[col].fillna(df_raw[col].mode()[0])

print(" Missing Values (After):\n", df_raw.isnull().sum())

numeric_cols = ['cost_of_the_order', 'food_preparation_time', 'delivery_time']
print(" Numerical Summary:\n", df_raw[numeric_cols].describe())
print(" Correlation Matrix:\n", df_raw[numeric_cols].corr())

day_avg = df_raw.groupby('day_of_the_week')['delivery_time'].mean().sort_values()
print(" Average Delivery Time by Day:\n", day_avg)

cuisine_stats = df_raw.groupby('cuisine_type')[numeric_cols].mean().sort_values('cost_of_the_order', ascending=False)
print(" Average Metrics by Cuisine Type (Top 10 by Cost):\n", cuisine_stats.head(10))

rating_counts = df_raw['rating'].value_counts()
print(" Rating Distribution:\n", rating_counts)

!pip install rapidfuzz

# HÜCRE 3 — RapidFuzz ile restoran eşleştirme ve MERGE

import pandas as pd
from rapidfuzz import process

food_df = pd.read_csv("/content/food_order.csv")
loc_df = pd.read_csv("/content/nyc_osm_restaurants.csv")

# Normalize
food_df["restaurant_name_clean"] = food_df["restaurant_name"].str.lower().str.strip()
loc_df["restaurant_name_clean"] = loc_df["restaurant_name"].str.lower().str.strip()

def find_best_match(name, choices, threshold=90):
    match = process.extractOne(name, choices, score_cutoff=threshold)
    return match[0] if match else None

loc_names = loc_df["restaurant_name_clean"].unique()
food_df["matched_name"] = food_df["restaurant_name_clean"].apply(lambda x: find_best_match(x, loc_names))

merged_df = pd.merge(
    food_df,
    loc_df[["restaurant_name_clean", "latitude", "longitude"]],
    left_on="matched_name",
    right_on="restaurant_name_clean",
    how="left"
)

merged_df.drop(columns=["restaurant_name_clean_x", "restaurant_name_clean_y", "matched_name"], inplace=True)
merged_df.to_csv("food_with_all_locations.csv", index=False)
print(" Dosya kaydedildi: food_with_all_locations.csv")

# ============= EDA A — MERGE SONRASI (tam burada!)
print("\n[EDA A] Merge sonrası hızlı kontrol")
print(">> Satır-sütun:", merged_df.shape)
print("\n>> İlk 5 satır:")
print(merged_df.head())
print("\n>> Sütun tipleri:")
print(merged_df.dtypes)
print("\n>> Eksik değer sayıları:")
missing_after_merge = merged_df.isna().sum().sort_values(ascending=False)
print(missing_after_merge)
missing_after_merge.to_csv(f"{outdir}/eda/missing_after_merge.csv")

# HÜCRE 4 — NYC OSM ile lokasyon doldurma + basit geo özellikler

import pandas as pd
import numpy as np

df = pd.read_csv("food_with_all_locations.csv")
osm_df = pd.read_csv("nyc_osm_restaurants.csv")

# Normalize
df["restaurant_name_clean"] = df["restaurant_name"].str.lower().str.strip()
osm_df["restaurant_name_clean"] = osm_df["restaurant_name"].str.lower().str.strip()

# Eksik lat/lon varsa osm'den doldur
df = df.merge(
    osm_df[["restaurant_name_clean", "latitude", "longitude"]],
    on="restaurant_name_clean",
    how="left",
    suffixes=('', '_osm')
)
df["latitude"] = df["latitude"].fillna(df["latitude_osm"])
df["longitude"] = df["longitude"].fillna(df["longitude_osm"])
df.drop(columns=["latitude_osm", "longitude_osm", "restaurant_name_clean"], inplace=True)

# Popüler mutfak türü
popular_cuisine = df["cuisine_type"].mode()[0]
df["is_popular_cuisine"] = (df["cuisine_type"] == popular_cuisine).astype(int)

# Gruplama (orijinalindeki gibi)
grouped = df.groupby(["order_id", "cuisine_type"], as_index=False).agg({
    "customer_id": "first",
    "restaurant_name": "first",
    "cost_of_the_order": "sum",
    "day_of_the_week": "first",
    "rating": "first",
    "food_preparation_time": "mean",
    "delivery_time": "mean",
    "latitude": "first",
    "longitude": "first",
    "is_popular_cuisine": "first"
})
grouped["order_count"] = df.groupby(["order_id", "cuisine_type"]).size().values
grouped.to_csv("final_food_dataset.csv", index=False)
print("Final dataset saved as: final_food_dataset.csv")

# HÜCRE 5 — Yol verisinden türetilmiş sütunlar + (LEAKAGE UYARISI)

import pandas as pd
import numpy as np

df = pd.read_csv("final_food_dataset.csv")
edges = pd.read_csv("nyc_edges.csv", low_memory=False)

edges["highway_str"] = edges["highway"].apply(lambda x: ', '.join(eval(x)) if isinstance(x, str) and x.startswith("[") else str(x))
avg_road_length = edges["length"].mean()
top_road_type = edges["highway_str"].value_counts().idxmax()

# Basit tahmini yol uzunluğu (sabit, düşük bilgi içeriyor ama kalsın)
df["estimated_road_length"] = avg_road_length

# Basit lojistik bayraklar (LEAKAGE OLMAYANLAR)
df["is_weekend"] = df["day_of_the_week"].apply(lambda x: 1 if str(x).lower() in ["weekend", "saturday", "sunday", "5", "6"] else 0)
df["rating_given_flag"] = (pd.to_numeric(df["rating"].replace("Not given", 0), errors="coerce").fillna(0) != 0).astype(int)
df["high_cost_flag"] = (df["cost_of_the_order"] > 25).astype(int)

# !!! DİKKAT: AŞAĞIDAKİLER HEDEF SIZINTISI YAPAR, MODEL GİRİŞİNE SOKMAYACAĞIZ !!!
# df["total_time"] = df["food_preparation_time"] + df["delivery_time"]
# df["order_cost_per_minute"] = df["cost_of_the_order"] / df["total_time"]
# df["delivery_efficiency"] = df["cost_of_the_order"] / (df["delivery_time"] + df["food_preparation_time"])

df.to_csv("enhanced_food_with_geo_features.csv", index=False)
print("Saved: enhanced_food_with_geo_features.csv")
print("Estimated avg road length (m):", avg_road_length)
print("Most common road type:", top_road_type)

# HÜCRE 6 — Eksik lokasyon geocoding (yavaş olabilir) + temizlik

import pandas as pd
import numpy as np
from geopy.geocoders import Nominatim
from time import sleep
from tqdm.notebook import tqdm

df = pd.read_csv("/content/enhanced_food_with_geo_features.csv")

geolocator = Nominatim(user_agent="geo_filler_nyc_esra")

def get_location(row):
    if pd.isnull(row['latitude']) or pd.isnull(row['longitude']):
        name = row['restaurant_name']
        query = f"{name}, New York, USA"
        try:
            location = geolocator.geocode(query, timeout=10)
            if location:
                return pd.Series([location.latitude, location.longitude])
        except:
            return pd.Series([np.nan, np.nan])
    return pd.Series([row['latitude'], row['longitude']])

tqdm.pandas()
df[['latitude', 'longitude']] = df.progress_apply(get_location, axis=1)
df.to_csv("/content/enhanced_geo_completed.csv", index=False)
print(" Missing locations filled where possible → enhanced_geo_completed.csv")

# =========================
# HÜCRE 7 — Son temizlik + leakage olmayan ekstra özellik
# =========================
import pandas as pd
from geopy.distance import geodesic

df = pd.read_csv("/content/enhanced_geo_completed.csv")

# Rating temizliği
df['rating'] = df['rating'].replace("Not given", 0)
df['rating'] = pd.to_numeric(df['rating'], errors='coerce').fillna(0)

# Eksik koordinatları düşür
df = df.dropna(subset=["latitude", "longitude"]).copy()

# Times Square'a uzaklık (km) — leakage değil
center_point = (40.7580, -73.9855)
df['distance_to_center'] = df.apply(lambda row: geodesic((row['latitude'], row['longitude']), center_point).km, axis=1)

df.to_csv("/content/enhanced_with_features.csv", index=False)
print("Clean data saved → enhanced_with_features.csv")
print(f"Remaining rows: {len(df)}")

# =========================
# HÜCRE 8 — EDA B (FEATURE ENGINEERING SONRASI)
# =========================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error

df_ready = pd.read_csv("/content/enhanced_with_features.csv")

# Sınıflandırma hedefi üret (≤25 dk)
df_ready["delivered_<=25min"] = (df_ready["delivery_time"] <= 25).astype(int)

# --- Hedef dağılımı
plt.figure(figsize=(10,4))
plt.hist(df_ready["delivery_time"].dropna(), bins=30)
plt.title("Delivery Time — Histogram")
plt.xlabel("minutes"); plt.ylabel("count")
plt.show()

# --- Class balance
cls_counts = df_ready["delivered_<=25min"].value_counts()
print("\n>> Class balance (0=later, 1=≤25min):")
print(df_ready["delivered_<=25min"].value_counts())
cls_counts.to_csv(f"{outdir}/eda/class_balance.csv")

# --- Baseline (train mean)
tmp_train, tmp_test = train_test_split(df_ready, test_size=0.2, random_state=42)

y_tr = tmp_train["delivery_time"].values
y_te = tmp_test["delivery_time"].values
train_mean = float(np.nanmean(y_tr))

def rmse(y_true, y_pred):
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))

baseline = {
    "train_MAE": float(mean_absolute_error(y_tr, np.full_like(y_tr, train_mean))),
    "test_MAE":  float(mean_absolute_error(y_te, np.full_like(y_te, train_mean))),
    "train_RMSE": rmse(y_tr, np.full_like(y_tr, train_mean)),  # <- karekökü biz alıyoruz
    "test_RMSE":  rmse(y_te, np.full_like(y_te, train_mean))
}

print("\n>> Baseline (train mean) metrics:")
print(baseline)
pd.Series(baseline).to_csv(f"{outdir}/eda/baseline_regression_mean.csv")

# HÜCRE 9 — MODELLEME (LEAKAGE-FREE) + METRİKLER

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from geopy.distance import geodesic
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
from xgboost import XGBRegressor
from sklearn.linear_model import Ridge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import json

df = pd.read_csv("/content/enhanced_with_features.csv")
df["delivered_<=25min"] = (df["delivery_time"] <= 25).astype(int)

# === SIZINTI YAPMAYAN model özellikleri ===
# (delivery_time veya ondan türetilenler kesinlikle yok!)
features = [
    'cost_of_the_order',
    'food_preparation_time',
    'is_weekend',
    'estimated_road_length',   # sabit ama kalsın
    'is_popular_cuisine',
    'is_chain',                # aşağıda üretmedikse 0 kabul edilebilir (opsiyonel)
    'distance_to_center',
    'rating_given_flag',
    'high_cost_flag'
]
# 'is_chain' önceki hücrede tanımlı değilse varsayılan ekleyelim:
if 'is_chain' not in df.columns:
    df['is_chain'] = 0

# NaN temizliği
for c in features + ['delivery_time']:
    df[c] = pd.to_numeric(df[c], errors='coerce')
df = df.dropna(subset=features + ['delivery_time']).copy()

X = df[features]
y = df['delivery_time']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# MODELLER (stacking dahil)
rf  = RandomForestRegressor(n_estimators=200, random_state=42)
gb  = GradientBoostingRegressor(n_estimators=200, random_state=42)
xgb = XGBRegressor(n_estimators=300, learning_rate=0.08, max_depth=6, subsample=0.9, colsample_bytree=0.9, random_state=42)

final_est = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha=1.0))
stack = StackingRegressor(
    estimators=[('rf', rf), ('gb', gb), ('xgb', xgb)],
    final_estimator=final_est,
    passthrough=True,
    n_jobs=-1
)

# Eğitim
rf.fit(X_train, y_train)
gb.fit(X_train, y_train)
xgb.fit(X_train, y_train)
stack.fit(X_train, y_train)

# Tahminler
rf_pred   = rf.predict(X_test)
gb_pred   = gb.predict(X_test)
xgb_pred  = xgb.predict(X_test)
stack_pred= stack.predict(X_test)

def scores(y_true, y_pred):
    return {
        "MAE": float(mean_absolute_error(y_true, y_pred)),
        "RMSE": float(np.sqrt(mean_squared_error(y_true, y_pred))),  # karekök ile RMSE
        "R2": float(r2_score(y_true, y_pred))
    }

reg_report = {
    "MeanModel": scores(y_test, np.full_like(y_test, y_train.mean())),
    "RandomForest": scores(y_test, rf_pred),
    "GradientBoosting": scores(y_test, gb_pred),
    "XGBoost": scores(y_test, xgb_pred),
    "Stacking": scores(y_test, stack_pred),
}

# Sınıflandırma metrikleri (≤25 dk)
y_true_cls = (y_test <= 25).astype(int)
y_pred_cls = (stack_pred <= 25).astype(int)  # stacking'e göre karar

cls_report = {
    "Accuracy": float(accuracy_score(y_true_cls, y_pred_cls)),
    "Precision": float(precision_score(y_true_cls, y_pred_cls)),
    "Recall": float(recall_score(y_true_cls, y_pred_cls)),
    "F1": float(f1_score(y_true_cls, y_pred_cls))
}

print("\nRegresyon karşılaştırma:")
print(reg_report)
print("\nSınıflandırma (Stacking ≤25dk):")
print(cls_report)
# ========== ACCURACY MATRIX / CONFUSION MATRICES ==========
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

os.makedirs(f"{outdir}/eda", exist_ok=True)

THRESHOLD = 25  # dakika
y_true_cls = (y_test <= THRESHOLD).astype(int)

preds_reg = {
    "RandomForest": rf_pred,
    "GradientBoosting": gb_pred,
    "XGBoost": xgb_pred,
    "Stacking": stack_pred
}

acc_table_rows = []
for name, yhat in preds_reg.items():
    y_pred_cls = (yhat <= THRESHOLD).astype(int)
    cm = confusion_matrix(y_true_cls, y_pred_cls, labels=[0,1])
    tn, fp, fn, tp = cm.ravel()
    acc = (tp + tn) / cm.sum()
    prec = tp / (tp + fp) if (tp + fp) else 0.0
    rec  = tp / (tp + fn) if (tp + fn) else 0.0
    f1   = (2*prec*rec)/(prec+rec) if (prec+rec) else 0.0

    # Kaydet: PNG
    plt.figure(figsize=(4.5,4))
    plt.imshow(cm, interpolation='nearest')
    plt.title(f'Confusion Matrix — {name}')
    plt.xticks([0,1], ["Pred>25", "Pred<=25"])
    plt.yticks([0,1], ["Actual>25", "Actual<=25"])
    for (i, j), v in np.ndenumerate(cm):
        plt.text(j, i, int(v), ha='center', va='center')
    plt.tight_layout()
    plt.savefig(f"{outdir}/eda/confusion_matrix_{name}.png", dpi=140)
    plt.close()

    # Tek tabloda özet
    acc_table_rows.append({
        "Model": name, "Accuracy": acc,
        "Precision": prec, "Recall": rec, "F1": f1,
        "TN": int(tn), "FP": int(fp), "FN": int(fn), "TP": int(tp)
    })

# Tüm modellerin sınıflandırma özeti (threshold=25 dak)
acc_df = pd.DataFrame(acc_table_rows).sort_values("Accuracy", ascending=False)
print("\n== Classification Summary (threshold=25 min) ==")
print(acc_df)
acc_df.to_csv(f"{outdir}/eda/classification_summary_threshold_{THRESHOLD}.csv", index=False)


# JSON + CSV çıktı
import pandas as pd, os
os.makedirs("/content/outputs", exist_ok=True)

with open("/content/outputs/metrics.json", "w") as f:
    json.dump({"regression": reg_report, "classification": cls_report}, f, indent=2)

rows = []
for name, m in reg_report.items():
    rows.append({"task":"regression","model":name, **m})
rows.append({"task":"classification","model":"Stacking→≤25", **cls_report})
pd.DataFrame(rows).to_csv("/content/outputs/metrics.csv", index=False)

print("\nÇıktılar → /content/outputs/metrics.json ve metrics.csv")

# ROC–AUC, PR–AUC ve EŞİK (dakika) OPTİMİZASYONU

from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score

os.makedirs(f"{outdir}/eda", exist_ok=True)

THRESHOLD_DEFAULT = 25  # mevcut karar eşiği (dakika)
y_true_cls = (y_test <= THRESHOLD_DEFAULT).astype(int)

# Regresyon çıktısı: dakika -> "daha küçük = daha iyi (pozitif sınıf)" mantığıyla
# skor tanımı: score = -yhat (ROC/PR için "daha büyük = daha pozitif" olmalı)
preds_reg = {
    "RandomForest": rf_pred,
    "GradientBoosting": gb_pred,
    "XGBoost": xgb_pred,
    "Stacking": stack_pred
}

roc_pr_rows = []
best_threshold_rows = []

for name, yhat in preds_reg.items():
    scores = -yhat.astype(float)

    # --- ROC-AUC & PR-AUC (Average Precision) ---
    try:
        roc_auc = roc_auc_score(y_true_cls, scores)
    except ValueError:
        roc_auc = float("nan")
    prec, rec, pr_thresh = precision_recall_curve(y_true_cls, scores)
    ap = average_precision_score(y_true_cls, scores)

    # ROC eğrisi
    fpr, tpr, roc_thresh = roc_curve(y_true_cls, scores)
    plt.figure(figsize=(5,4))
    plt.plot(fpr, tpr, lw=2, label=f"AUC={roc_auc:.3f}")
    plt.plot([0,1],[0,1],'--', lw=1)
    plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title(f"ROC — {name}")
    plt.legend(loc="lower right")
    plt.tight_layout()
    plt.savefig(f"{outdir}/eda/roc_{name}.png", dpi=140)
    plt.close()

    # PR eğrisi
    plt.figure(figsize=(5,4))
    plt.plot(rec, prec, lw=2, label=f"AP={ap:.3f}")
    plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title(f"PR — {name}")
    plt.legend(loc="lower left")
    plt.tight_layout()
    plt.savefig(f"{outdir}/eda/pr_{name}.png", dpi=140)
    plt.close()

    roc_pr_rows.append({
        "Model": name,
        "ROC_AUC": float(roc_auc),
        "PR_AP": float(ap)
    })

    # --- En iyi dakika eşiğini (≤ T dakika ⇒ pozitif) F1'e göre bul ---
    # Tarama: tahminlerin min-max aralığında 300 eşik
    t_min, t_max = float(np.min(yhat)), float(np.max(yhat))
    candidate_T = np.linspace(t_min, t_max, 300)

    best = {"F1": -1, "Precision": 0, "Recall": 0, "Accuracy": 0, "TP":0,"FP":0,"FN":0,"TN":0, "T_minutes": THRESHOLD_DEFAULT}
    for T in candidate_T:
        pred_cls = (yhat <= T).astype(int)
        cm = confusion_matrix(y_true_cls, pred_cls, labels=[0,1])
        tn, fp, fn, tp = cm.ravel()
        acc = (tp + tn) / cm.sum() if cm.sum() else 0.0
        prec_ = tp / (tp + fp) if (tp + fp) else 0.0
        rec_  = tp / (tp + fn) if (tp + fn) else 0.0
        f1_   = (2*prec_*rec_)/(prec_+rec_) if (prec_+rec_) else 0.0
        if f1_ > best["F1"]:
            best = {
                "F1": f1_, "Precision": prec_, "Recall": rec_, "Accuracy": acc,
                "TP": int(tp), "FP": int(fp), "FN": int(fn), "TN": int(tn),
                "T_minutes": float(T)
            }

    best_threshold_rows.append({"Model": name, **best})

# Özet tabloları yazdır + kaydet
roc_pr_df = pd.DataFrame(roc_pr_rows).sort_values("ROC_AUC", ascending=False)
best_thr_df = pd.DataFrame(best_threshold_rows).sort_values("F1", ascending=False)

print("\n== ROC/PR Özeti ==")
print(roc_pr_df)
print("\n== En İyi Eşik (dakika) — F1 maks. ==")
print(best_thr_df)

roc_pr_df.to_csv(f"{outdir}/eda/roc_pr_summary.csv", index=False)
best_thr_df.to_csv(f"{outdir}/eda/best_threshold_by_f1.csv", index=False)

# =========================
# HÜCRE 10 — SHAP (opsiyonel görselleştirme)
# =========================
import shap
import pandas as pd

# XGBoost için SHAP
explainer = shap.Explainer(xgb, X_test)
shap_values = explainer(X_test)
shap.plots.beeswarm(shap_values)

# =========================
# HÜCRE 11 — Clustering (orijinal kısmın, küçük düzeltmelerle)
# UYARI: DBSCAN/OPTICS'te eps/metrik coğrafi birimlere göre ayarlanmalı
# =========================
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift, OPTICS
import hdbscan

dfc = pd.read_csv("/content/enhanced_with_features.csv")
coords = dfc[['latitude', 'longitude']].dropna()

# KMeans
kmeans = KMeans(n_clusters=4, random_state=42, n_init="auto")
dfc.loc[coords.index, 'kmeans_cluster'] = kmeans.fit_predict(coords)

# DBSCAN (derece yerine küçük eps değerleri dene; coğrafide haversine daha doğru olur)
dbscan = DBSCAN(eps=0.02, min_samples=10)
dfc.loc[coords.index, 'dbscan_cluster'] = dbscan.fit_predict(coords)

# HDBSCAN
hdb = hdbscan.HDBSCAN(min_cluster_size=10)
dfc.loc[coords.index, 'hdbscan_cluster'] = hdb.fit_predict(coords)

# Agglomerative
agglo = AgglomerativeClustering(n_clusters=4)
dfc.loc[coords.index, 'agglo_cluster'] = agglo.fit_predict(coords)

# MeanShift
meanshift = MeanShift()
dfc.loc[coords.index, 'meanshift_cluster'] = meanshift.fit_predict(coords)

# OPTICS
optics = OPTICS(min_samples=10)
dfc.loc[coords.index, 'optics_cluster'] = optics.fit_predict(coords)

# Görseller
cluster_columns = [
    ('kmeans_cluster', 'KMeans'),
    ('dbscan_cluster', 'DBSCAN'),
    ('hdbscan_cluster', 'HDBSCAN'),
    ('agglo_cluster', 'Agglomerative'),
    ('meanshift_cluster', 'MeanShift'),
    ('optics_cluster', 'OPTICS')
]

for col, title in cluster_columns:
    plt.figure(figsize=(10,6))
    sns.scatterplot(data=dfc, x="longitude", y="latitude", hue=col, palette="tab10", s=30)
    plt.title(f"{title} Geographic Clustering")
    plt.grid(True)
    plt.show()

# =========================
# HÜCRE X — Kümeleme Metrikleri Karşılaştırması
# =========================
import os
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift, OPTICS
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# HDBSCAN (yüklü değilse önce: !pip install hdbscan)
import hdbscan

# Çıkış klasörü
outdir = "/content/outputs"
os.makedirs(f"{outdir}/eda", exist_ok=True)

# Veriyi oku (notebook boyunca üretilen dosya yolu)
csv_candidates = ["/content/enhanced_with_features.csv", "enhanced_with_features.csv"]
for p in csv_candidates:
    if os.path.exists(p):
        data_path = p
        break
else:
    raise FileNotFoundError("enhanced_with_features.csv bulunamadı.")

dfc = pd.read_csv(data_path)

# Koordinatlar
coords = dfc[['latitude', 'longitude']].dropna()
if coords.empty:
    raise ValueError("Koordinat verisi boş. 'latitude' ve 'longitude' sütunlarını kontrol edin.")

# Modeller
models = {
    "KMeans(n=4)": KMeans(n_clusters=4, random_state=42, n_init="auto"),
    "DBSCAN(eps=0.02,min=10)": DBSCAN(eps=0.02, min_samples=10),
    "HDBSCAN(min_cluster_size=10)": hdbscan.HDBSCAN(min_cluster_size=10),
    "Agglomerative(n=4)": AgglomerativeClustering(n_clusters=4),
    "MeanShift": MeanShift(),
    "OPTICS(min=10)": OPTICS(min_samples=10)
}

def eval_metrics(X, labels, drop_noise=True):
    """ Gürültüyü (-1) dışarıda tutup metrikleri hesapla. """
    labels = np.asarray(labels)
    if drop_noise and (-1 in labels):
        mask = labels != -1
        X, labels = X[mask], labels[mask]
    # Geçerli skor koşulları: en az 2 farklı cluster ve tüm noktalar tekil değil
    if len(X) < 2 or len(set(labels)) < 2 or len(set(labels)) == len(labels):
        return np.nan, np.nan, np.nan, 0, int((labels == -1).sum())
    try:
        sil = silhouette_score(X, labels)
    except Exception:
        sil = np.nan
    try:
        db  = davies_bouldin_score(X, labels)
    except Exception:
        db = np.nan
    try:
        ch  = calinski_harabasz_score(X, labels)
    except Exception:
        ch = np.nan
    k = len(set(labels)) - (1 if -1 in labels and drop_noise else 0)
    noise = int((labels == -1).sum())
    return sil, db, ch, k, noise

results = []
labels_dict = {}

for name, model in models.items():
    try:
        y = model.fit_predict(coords.values)
        labels_dict[name] = y  # İstersen haritada boyamak için saklıyoruz
        sil, db, ch, k, noise = eval_metrics(coords.values, y, drop_noise=True)
        results.append({
            "Method": name,
            "Num_Clusters": k,
            "Num_Noise": noise,
            "Silhouette(↑)": sil,
            "Davies-Bouldin(↓)": db,
            "Calinski-Harabasz(↑)": ch
        })
    except Exception as e:
        results.append({
            "Method": name,
            "Num_Clusters": 0,
            "Num_Noise": 0,
            "Silhouette(↑)": np.nan,
            "Davies-Bouldin(↓)": np.nan,
            "Calinski-Harabasz(↑)": np.nan,
            "Error": str(e)
        })

metrics_df = pd.DataFrame(results)

# Sıralı özet: Silhouette'e göre azalan, eşitlikte DB'ye göre artan
metrics_df_sorted = metrics_df.sort_values(
    by=["Silhouette(↑)", "Davies-Bouldin(↓)"],
    ascending=[False, True],
    na_position="last"
).reset_index(drop=True)

print("\n== Kümeleme Karşılaştırma Metrikleri ==")
print(metrics_df_sorted)

# Kaydet
save_path = f"{outdir}/eda/clustering_metrics.csv"
metrics_df_sorted.to_csv(save_path, index=False)
print(f"\nTablo kaydedildi → {save_path}")

# (İsteğe bağlı) En iyi görüneni yazdır
best_row = metrics_df_sorted.iloc[0]
print(f"\nEn iyi (heuristic): {best_row['Method']} | "
      f"Sil={best_row['Silhouette(↑)']:.3f} | "
      f"DB={best_row['Davies-Bouldin(↓)']:.3f} | "
      f"CH={best_row['Calinski-Harabasz(↑)']:.1f} | "
      f"Küme={int(best_row['Num_Clusters'])} | Noise={int(best_row['Num_Noise'])}")
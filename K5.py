# -*- coding: utf-8 -*-
"""EFAPSONN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d_HplxkbED2wj3cH-2A4yCVvmVCcl2vy
"""

# [1] Imports + Ortam
import os, re, warnings, datetime as dt
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score, adjusted_rand_score

# Çıktı klasörü
OUTDIR = "outputs"
os.makedirs(OUTDIR, exist_ok=True)

print("OK: ortam hazır, OUTDIR =", OUTDIR)

# [2] Konfig: CSV dosya yolları (gerekirse değiştir)
EFAP_CSV = "EFAP_pdf_11_4_24.csv"
NH_CSV   = "Neighborhood_Prioritization_Map_2024.csv"
FLW_CLEAN_CSV = "flw_cleaned.csv"   # FLW temiz çıktı (US loss% çıkaracağız)
STK_CSV = "STK.csv"


print("EFAP_CSV:", EFAP_CSV)
print("NH_CSV:", NH_CSV, "(var mı:", os.path.exists(NH_CSV), ")")

# [3] Yardımcılar (tek sürüm)
STK_CSV ="/content/STK.csv"  # Bağış/STK listesi yoksa None bırak
WEEK_DAYS = ["MON","TUE","WED","THU","FRI","SAT","SUN"]

def _extract_day(token: str):
    m = {
        'M':'MON','MON':'MON','MONDAY':'MON',
        'TU':'TUE','TUES':'TUE','TUE':'TUE','TUESDAY':'TUE',
        'W':'WED','WE':'WED','WED':'WED','WEDNESDAY':'WED',
        'TH':'THU','THU':'THU','THUR':'THU','THURS':'THU','THURSDAY':'THU',
        'F':'FRI','FRI':'FRI','FRIDAY':'FRI',
        'SA':'SAT','SAT':'SAT','SATURDAY':'SAT',
        'SU':'SUN','SUN':'SUN','SUNDAY':'SUN'
    }
    t = str(token).upper().replace('–','-').replace('\u00A0',' ').strip()
    t = re.sub(r'\([^)]*\)', '', t)
    t = t.split()[0] if ' ' in t else t
    t = re.sub(r'[^A-Z-]', '', t)
    if t in m: return m[t]
    for std in WEEK_DAYS:
        if t.startswith(std): return std
    return None

def expand_day_range(day_range: str):
    s = str(day_range).upper().replace('–','-').strip()
    if '-' not in s:
        d = _extract_day(s)
        return [d] if d else []
    left, right = [p.strip() for p in s.split('-', 1)]
    start, end  = _extract_day(left), _extract_day(right)
    if not start or not end: return []
    i1, i2 = WEEK_DAYS.index(start), WEEK_DAYS.index(end)
    if i2 < i1: i2 += 7
    return [WEEK_DAYS[i % 7] for i in range(i1, i2+1)]

def parse_weeks(text):
    if not text: return [1,2,3,4]
    s = str(text).upper()
    nums = re.findall(r'\d+', s)
    return [int(n) for n in nums] if nums else [1,2,3,4]

def parse_hours(hours_str):
    s = str(hours_str).replace('–','-').replace('—','-')
    s = re.sub(r'\bto\b', '-', s, flags=re.I)        # "10am to 2pm" -> "10am-2pm"
    parts = re.split(r'[,&/;]+', s)                  # , & / ; ayır
    return [p.strip().upper() for p in parts if p.strip()]

def parse_days_hours(text):
    t = str(text).upper().replace('–','-').replace('\u00A0',' ')
    m = re.match(r'([-A-Z,/\s]+)(\([^)]+\))?\s+(.+)', t, re.I)  # '-' sınıfın başında
    if not m: return ([], [], [])
    day_part  = m.group(1).strip()
    week_part = m.group(2)
    hours_part= m.group(3).strip()

    days = []
    for d in re.split(r'[,/]', day_part):
        d = d.strip()
        if '-' in d: days.extend(expand_day_range(d))
        else:
            dd = _extract_day(d)
            if dd: days.append(dd)

    weeks = parse_weeks(week_part)
    hours = parse_hours(hours_part)
    order = {v:i for i,v in enumerate(WEEK_DAYS)}
    return (sorted(set(days), key=lambda x: order.get(x, 999)), weeks, hours)

def _valid_latlon(lat, lon):
    try:
        return (-90 <= float(lat) <= 90) and (-180 <= float(lon) <= 180)
    except Exception:
        return False

def _eda_quality(df: pd.DataFrame, name: str, outdir: str):
    import json
    def safe_nunique(s: pd.Series) -> int:
        try:
            return int(s.nunique(dropna=True))
        except TypeError:
            s2 = s.apply(lambda x: json.dumps(x, ensure_ascii=False, sort_keys=True)
                                 if isinstance(x, (list, dict)) else x)
            return int(pd.Series(s2).nunique(dropna=True))

    rows=[]
    for c in df.columns:
        s = df[c]
        rows.append({
            "column": c,
            "dtype": str(s.dtype),
            "null_rate": float(pd.isna(s).mean()),
            "nunique": safe_nunique(s),
        })
    q = pd.DataFrame(rows).sort_values("null_rate", ascending=False)
    q.to_csv(os.path.join(outdir, f"{name}_quality_summary.csv"), index=False)
    print(f"[EDA] {name}_quality_summary.csv kaydedildi")
    return q

def _save_hist(series: pd.Series, outdir: str, fname: str, bins=30):
    import matplotlib.pyplot as plt
    s = pd.to_numeric(series, errors="coerce").dropna()
    if s.empty: return
    plt.figure()
    s.hist(bins=bins)
    plt.title(fname)
    plt.tight_layout()
    plt.savefig(os.path.join(outdir, f"eda_hist_{fname}.png"), dpi=120)
    plt.close()

print("OK: yardımcı fonksiyonlar yüklendi.")
# ========== [ADD] FLW → NH zenginleştirme: us_loss_pct ==========
def _merge_flw_us_loss(nh_df: pd.DataFrame, flw_path: str = FLW_CLEAN_CSV):
    if nh_df is None or nh_df.empty or not os.path.exists(flw_path):
        return nh_df, np.nan

    flw = pd.read_csv(flw_path)

    # ABD satırlarını esnek filtre
    mask_us = pd.Series(False, index=flw.index)
    for c in flw.columns:
        if flw[c].dtype == "O":
            mask_us |= flw[c].astype(str).str.contains("United States|America", case=False, na=False, regex=True)
    flw_us = flw[mask_us].copy()

    # yıl / loss%
    flw_us["year"] = pd.to_numeric(flw_us.get("year", np.nan), errors="coerce").astype("Int64")
    flw_us["loss_percentage"] = pd.to_numeric(flw_us.get("loss_percentage", np.nan), errors="coerce")

    # yıla göre ABD median loss%
    flw_year = (flw_us.dropna(subset=["loss_percentage"])
                      .groupby("year", dropna=True)["loss_percentage"]
                      .median().rename("us_loss_pct").reset_index())
    latest = flw_year["us_loss_pct"].iloc[-1] if not flw_year.empty else np.nan

    # NH'de yıl varsa yıla göre merge; yoksa son değeri yayınla
    nh2 = nh_df.copy()
    year_col = next((c for c in ["year","YEAR","Year"] if c in nh2.columns), None)
    if year_col:
        nh2[year_col] = pd.to_numeric(nh2[year_col], errors="coerce").astype("Int64")
        nh2 = nh2.merge(flw_year, left_on=year_col, right_on="year", how="left")
        nh2.drop(columns=["year"], inplace=True, errors="ignore")
        if "us_loss_pct" in nh2.columns and pd.notna(latest):
            nh2["us_loss_pct"] = nh2["us_loss_pct"].fillna(latest)
    else:
        nh2["us_loss_pct"] = latest

    return nh2, latest


# ========== [ADD] 5.5 Bağış/STK: bağış öncelik skoru + (opsiyonel) STK öneri ==========
def _zminmax(s: pd.Series):
    s = pd.to_numeric(s, errors="coerce")
    vmin, vmax = s.min(skipna=True), s.max(skipna=True)
    if pd.isna(vmin) or pd.isna(vmax) or vmin == vmax:
        return pd.Series(np.nan, index=s.index)
    return (s - vmin) / (vmax - vmin)

def _compute_donation_priority(nh_df: pd.DataFrame, stk_path: str = STK_CSV, outdir: str = OUTDIR):
    if nh_df is None or nh_df.empty:
        return nh_df, None

    df = nh_df.copy()
    for c in ["need_score","service_score","PRIORITY_SCORE","us_loss_pct"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # Bileşenler: yüksek ihtiyaç(+), düşük hizmet(+), FLW makro(+)
    need = df["PRIORITY_SCORE"] if "PRIORITY_SCORE" in df.columns else df.get("need_score", pd.Series(np.nan, index=df.index))
    svc  = df.get("service_score", pd.Series(np.nan, index=df.index))
    flw  = df.get("us_loss_pct", pd.Series(np.nan, index=df.index))

    need_n = _zminmax(need)
    svc_inv = 1 - _zminmax(svc)
    flw_n  = _zminmax(flw)

    w_need, w_svc, w_flw = 0.5, 0.3, 0.2
    score = (w_need * need_n.fillna(need_n.median())) + \
            (w_svc  * svc_inv.fillna(svc_inv.median())) + \
            (w_flw  * flw_n.fillna(flw_n.median()))

    df["DONATION_PRIORITY_SCORE"] = score
    df.to_csv(os.path.join(outdir, "nh_with_donation_priority.csv"), index=False)
    suggestions = None

    # (Opsiyonel) STK önerileri — BORO bazlı eşleşme + kapasite
    if os.path.exists(stk_path) and "BORO" in df.columns:
        try:
            stk = pd.read_csv(stk_path)
            for c in stk.columns:
                if stk[c].dtype == "O":
                    stk[c] = stk[c].astype(str).str.strip()

            boro_need = (df.groupby("BORO", as_index=False)["DONATION_PRIORITY_SCORE"]
                           .mean().rename(columns={"DONATION_PRIORITY_SCORE":"BORO_NEED"}))
            key = "borough" if "borough" in stk.columns else ("BORO" if "BORO" in stk.columns else None)
            if key:
                suggestions = stk.merge(boro_need, left_on=key, right_on="BORO", how="left")
                cap_n = _zminmax(pd.to_numeric(suggestions["capacity_daily"], errors="coerce")) if "capacity_daily" in suggestions.columns else pd.Series(np.nan, index=suggestions.index)
                suggestions["ORG_PRIORITY"] = 0.7 * suggestions["BORO_NEED"].fillna(suggestions["BORO_NEED"].median()) + \
                                              0.3 * cap_n.fillna(cap_n.median())
                suggestions = suggestions.sort_values(["BORO","ORG_PRIORITY"], ascending=[True, False])
                suggestions.to_csv(os.path.join(outdir, "donation_org_suggestions.csv"), index=False)
        except Exception as e:
            print("[STK] okunamadı/işlenemedi:", e)

    print("[5.5] DONATION_PRIORITY_SCORE üretildi → outputs/nh_with_donation_priority.csv")
    if suggestions is not None:
        print("[5.5] donation_org_suggestions.csv yazıldı")

    return df, suggestions

# [4] Veri yükleme + preprocess (EFAP/NH)

if not os.path.exists(EFAP_CSV):
    raise FileNotFoundError(f"{EFAP_CSV} bulunamadı!")
efap = pd.read_csv(EFAP_CSV)
nh   = pd.read_csv(NH_CSV) if os.path.exists(NH_CSV) else pd.DataFrame()

# EFAP: DAYS güvenli parse -> list kolonlarını üret
if "DAYS" not in efap.columns:
    efap["DAYS"] = ""
if "DAYS_LIST" not in efap.columns:
    trip = efap["DAYS"].fillna("").apply(parse_days_hours)
    efap[["DAYS_LIST","WEEKS_LIST","HOURS_LIST"]] = pd.DataFrame(trip.tolist(), index=efap.index)

# NH: lat/lon güvenliği (varsa)
if not nh.empty and {'Latitude (generated)','Longitude (generated)'}.issubset(nh.columns):
    nh["Latitude (generated)"]  = pd.to_numeric(nh["Latitude (generated)"], errors="coerce")
    nh["Longitude (generated)"] = pd.to_numeric(nh["Longitude (generated)"], errors="coerce")
    nh = nh[nh.apply(lambda r: _valid_latlon(r["Latitude (generated)"], r["Longitude (generated)"]), axis=1)]

# (Opsiyonel) Temizlenmiş csv yaz — reproducibility
efap.to_csv(os.path.join(OUTDIR, "efap_cleaned.csv"), index=False)
if not nh.empty:
    nh.to_csv(os.path.join(OUTDIR, "nh_cleaned.csv"), index=False)

# [ADD] FLW (US loss%) ile NH'yi zenginleştir
nh, latest_us_loss = _merge_flw_us_loss(nh, FLW_CLEAN_CSV)
if "us_loss_pct" in nh.columns:
    nh.to_csv(os.path.join(OUTDIR, "nh_enriched_with_flw.csv"), index=False)
    msg = f"[FLW] us_loss_pct eklendi. Son ABD değeri ~ {latest_us_loss:.3f}" if pd.notna(latest_us_loss) else "[FLW] us_loss_pct eklendi."
    print(msg)

print("[Preprocess] EFAP ve NH temizlendi, cleaned CSV yazıldı.")

# [5] EDA

_ = _eda_quality(efap, "efap", OUTDIR)
if not nh.empty: _ = _eda_quality(nh, "nh", OUTDIR)

# EFAP metrik histogramları (varsa)
for col, tag in [
    ("OPEN_DAYS_PER_WEEK","efap_OPEN_DAYS_PER_WEEK"),
    ("WEEKLY_OPEN_HOURS","efap_WEEKLY_OPEN_HOURS"),
    ("service_score_raw","efap_service_score_raw"),
    ("TOTAL_DAYS_PER_MONTH","efap_TOTAL_DAYS_PER_MONTH")
]:
    if col in efap.columns: _save_hist(efap[col], OUTDIR, tag)

# NH skor histogramları (varsa)
for c in ["need_score","service_score","PRIORITY_SCORE"]:
    if c in nh.columns: _save_hist(nh[c], OUTDIR, f"nh_{c}")

# DAYS parse tanılama
if "DAYS" in efap.columns and {"DAYS_LIST","WEEKS_LIST","HOURS_LIST"}.issubset(efap.columns):
    ok_mask = efap["DAYS_LIST"].apply(lambda x: isinstance(x, list) and len(x) > 0)
    diag = efap.assign(PARSE_OK=ok_mask).groupby("PARSE_OK").size().rename("count").reset_index()
    diag.to_csv(os.path.join(OUTDIR, "efap_days_parse_summary.csv"), index=False)
    efap.loc[~ok_mask, ["PROGRAM","DAYS"]].head(50).to_csv(
        os.path.join(OUTDIR, "efap_days_parse_examples_bad.csv"), index=False)

print("[EDA] tamam.")

# [6] STEP 7 — MODELLEME: KMeans + Geo-DBSCAN

# --- KMeans (özellik seti) ---
if not nh.empty:
    clust_features = [
        "need_score","service_score","efap_sites","total_days",
        "Food.Insecure.Percentage","Unemployment.Rate",
        "Vulnerable.Population.Percentage","Sg Abv Ca"
    ]
    X = nh.copy()
    if "NTA" not in X.columns: X["NTA"] = X.index.astype(str)

    for c in clust_features:
        if c not in X.columns: X[c] = np.nan
        X[c] = pd.to_numeric(X[c], errors="coerce")
        med = X[c].median() if not np.isnan(pd.to_numeric(X[c], errors="coerce").median()) else 0.0
        X[c] = X[c].fillna(med)
        # [ADD] us_loss_pct varsa özelliklere dahil et
    if "us_loss_pct" in nh.columns and "us_loss_pct" not in clust_features:
        clust_features = clust_features + ["us_loss_pct"]


    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X[clust_features])

    best_k, best_score, best_labels = None, -1, None
    for k in [3,4,5,6]:
        try:
            km = KMeans(n_clusters=k, n_init=20, random_state=42)
            labels = km.fit_predict(X_scaled)
            sil = silhouette_score(X_scaled, labels) if len(set(labels)) > 1 else -1
            if sil > best_score:
                best_k, best_score, best_labels = k, sil, labels
        except Exception as e:
            print(f"[KMeans] k={k} hata:", e)

    if best_labels is not None:
        nh["KMEANS_CLUSTER"] = best_labels
        # Dinamik profil
        if "NTA" not in nh.columns:            nh["NTA"] = nh.index.astype(str)
        if "need_score" not in nh.columns:     nh["need_score"] = np.nan
        if "service_score" not in nh.columns:  nh["service_score"] = np.nan
        if "PRIORITY_SCORE" not in nh.columns: nh["PRIORITY_SCORE"] = np.nan

        agg_kwargs = dict(count=("NTA", "count"))
        if nh["PRIORITY_SCORE"].notna().any(): agg_kwargs["mean_priority"] = ("PRIORITY_SCORE","mean")
        if nh["need_score"].notna().any():     agg_kwargs["mean_need"]      = ("need_score","mean")
        if nh["service_score"].notna().any():  agg_kwargs["mean_service"]   = ("service_score","mean")

        kmeans_profile = nh.groupby("KMEANS_CLUSTER", as_index=False).agg(**agg_kwargs)
        nh.to_csv(os.path.join(OUTDIR, "nh_with_clusters.csv"), index=False)
        kmeans_profile.to_csv(os.path.join(OUTDIR, "kmeans_cluster_profile.csv"), index=False)
        print(f"[KMeans] seçilen k = {best_k}, silhouette = {best_score:.3f}")
    else:
        print("[KMeans] etiket üretilemedi.")

# --- Geo-DBSCAN (coverage-gap) ---
if not nh.empty and {"service_score","PRIORITY_SCORE","Latitude (generated)","Longitude (generated)"}.issubset(nh.columns):
    nh["service_low_flag"]   = nh["service_score"] < nh["service_score"].median()
    nh["priority_high_flag"] = nh["PRIORITY_SCORE"] > nh["PRIORITY_SCORE"].median()
    gap_geo = nh.loc[
        nh["service_low_flag"] & nh["priority_high_flag"],
        ["NTA","NTA.Name","Latitude (generated)","Longitude (generated)","PRIORITY_SCORE"]
    ].dropna()
    if not gap_geo.empty:
        coords = np.radians(gap_geo[["Latitude (generated)","Longitude (generated)"]].to_numpy())
        db = DBSCAN(eps=2.0/6371.0088, min_samples=3, metric="haversine")
        gap_geo = gap_geo.assign(DBSCAN_CLUSTER=db.fit_predict(coords))
        nh = nh.merge(gap_geo[["NTA","DBSCAN_CLUSTER"]], on="NTA", how="left")
        gap_geo.to_csv(os.path.join(OUTDIR, "dbscan_hotspots_points.csv"), index=False)
        print("[DBSCAN] hotspot noktaları yazıldı.")
    else:
        print("[DBSCAN] coverage gap boş.")

print("[STEP 7] modelleme tamam.")
# =========================
# 5.5 — Bağış/STK entegrasyonu (sadece harici STK CSV ile)
# =========================
import pandas as pd, numpy as np, os

# --- BORO sütununu garantiye al ---
def _ensure_boro(nh_df: pd.DataFrame) -> pd.DataFrame:
    df = nh_df.copy()
    for alt in ["BORO", "Borough", "borough", "boro"]:
        if alt in df.columns:
            if alt != "BORO":
                df["BORO"] = df[alt]
            break
    if "BORO" not in df.columns:
        boro_map = {"BX": "Bronx", "BK": "Brooklyn", "MN": "Manhattan",
                    "QN": "Queens", "SI": "Staten Island"}
        if "NTA" in df.columns:
            pref = df["NTA"].astype(str).str[:2]
            df["BORO"] = pref.map(boro_map).fillna("OTHER")
        else:
            df["BORO"] = "OTHER"
    df["BORO"] = df["BORO"].astype(str).str.strip()
    return df

def _zminmax_55(s: pd.Series):
    s = pd.to_numeric(s, errors="coerce")
    vmin, vmax = s.min(skipna=True), s.max(skipna=True)
    if pd.isna(vmin) or pd.isna(vmax) or vmin == vmax:
        return pd.Series(np.nan, index=s.index)
    return (s - vmin) / (vmax - vmin)

# --- BORO normalizasyonu (gelişmiş) ---
def _normalize_boro_55(x: str):
    if not isinstance(x, str): return "OTHER"
    t = x.strip().upper()
    if t in {"BX","BRONX"}: return "Bronx"
    if t in {"BK","BROOKLYN"}: return "Brooklyn"
    if t in {"MN","MANHATTAN"}: return "Manhattan"
    if t in {"QN","QUEENS"}: return "Queens"
    if t in {"SI"} or "STATEN" in t: return "Staten Island"
    return x.strip() if x.strip() else "OTHER"

def _load_stk_55_from_file(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"[STK] bulunamadı: {path}")
    stk = pd.read_csv(path)

    ren = {}
    for k, alts in {
        "org_id": ["org_id","id","OrgID","Org_Id"],
        "org_name": ["org_name","name","OrgName","Organization"],
        "borough": ["borough","BORO","Borough"],
        "categories": ["categories","category","cats"],
        "accepts_hot": ["accepts_hot","hot"],
        "accepts_cold": ["accepts_cold","cold"],
        "accepts_packed": ["accepts_packed","packed","packaged"],
        "accepts_bulk": ["accepts_bulk","bulk"],
        "capacity_daily": ["capacity_daily","capacity","daily_capacity"],
        "open_days": ["open_days","days"],
        "open_time": ["open_time","start_time"],
        "close_time": ["close_time","end_time"],
        "address": ["address","addr"],
        "contact": ["contact","email","phone"]
    }.items():
        for a in alts:
            if a in stk.columns:
                ren[a] = k
                break
    stk = stk.rename(columns=ren)

    if "borough" not in stk.columns:
        raise ValueError("[STK] 'borough' kolonu yok!")
    if "capacity_daily" not in stk.columns:
        stk["capacity_daily"] = 0

    for c in stk.columns:
        if stk[c].dtype == "O":
            stk[c] = stk[c].astype(str).str.strip()

    for c in ["accepts_hot","accepts_cold","accepts_packed","accepts_bulk"]:
        if c not in stk.columns: stk[c] = 0
        stk[c] = pd.to_numeric(stk[c], errors="coerce").fillna(0).astype(int)
    stk["capacity_daily"] = pd.to_numeric(stk["capacity_daily"], errors="coerce").fillna(0).astype(int)

    stk["BORO"] = stk["borough"].apply(_normalize_boro_55)
    if "categories" not in stk.columns: stk["categories"] = ""
    stk["_catset"] = stk["categories"].str.lower().apply(
        lambda s: set([t.strip() for t in str(s).split(",") if t.strip()])
    )
    return stk

def compute_donation_priority_55(nh_df: pd.DataFrame) -> pd.DataFrame:
    if nh_df is None or nh_df.empty: return nh_df, pd.DataFrame(columns=["BORO","BORO_NEED"])
    df = nh_df.copy()
    for c in ["need_score","service_score","PRIORITY_SCORE","us_loss_pct"]:
        if c in df.columns: df[c] = pd.to_numeric(df[c], errors="coerce")

    need = df["PRIORITY_SCORE"] if "PRIORITY_SCORE" in df.columns else df.get("need_score", np.nan)
    svc  = df.get("service_score", np.nan)
    flw  = df.get("us_loss_pct", np.nan)

    need_n = _zminmax_55(need)
    svc_inv = 1 - _zminmax_55(svc)
    flw_n  = _zminmax_55(flw)

    score = 0.5*need_n.fillna(need_n.median()) + 0.3*svc_inv.fillna(svc_inv.median()) + 0.2*flw_n.fillna(flw_n.median())
    df["DONATION_PRIORITY_SCORE"] = score

    boro_need = (df.groupby("BORO", as_index=False)["DONATION_PRIORITY_SCORE"]
                   .mean().rename(columns={"DONATION_PRIORITY_SCORE":"BORO_NEED"}))

    out_path = os.path.join(OUTDIR, "nh_with_donation_priority.csv")
    df.to_csv(out_path, index=False)
    print(f"[5.5] DONATION_PRIORITY_SCORE → {out_path}")
    return df, boro_need

# ---- ÇALIŞTIR ----
stk_df = _load_stk_55_from_file(STK_CSV)
nh = _ensure_boro(nh)                     # <<< BORO sütunu garanti
nh, boro_need_55 = compute_donation_priority_55(nh)

# [7] Ek değerlendirme: KMeans k-grid + bootstrap stability, DBSCAN eps sweep

def kmeans_grid_eval(X_scaled, k_grid=(3,4,5,6), repeats=5, seed=42):
    rows=[]
    for k in k_grid:
        sils=[]
        for r in range(repeats):
            km = KMeans(n_clusters=k, n_init=20, random_state=seed+r)
            labels = km.fit_predict(X_scaled)
            sils.append(silhouette_score(X_scaled, labels) if len(set(labels))>1 else -1)
        rows.append({"k":k, "silhouette_mean": float(np.mean(sils)), "silhouette_std": float(np.std(sils))})
    df = pd.DataFrame(rows)
    df.to_csv(os.path.join(OUTDIR, "kmeans_grid_eval.csv"), index=False)
    print("[KMeans] kmeans_grid_eval.csv kaydedildi")
    return df

def kmeans_bootstrap_stability(X_scaled, k, B=20, frac=0.8, seed=42):
    rng = np.random.RandomState(seed)
    subs = []
    n = X_scaled.shape[0]
    if n < 10: return float("nan")
    for b in range(B):
        idx = rng.choice(np.arange(n), size=max(5, int(frac*n)), replace=False)
        km = KMeans(n_clusters=k, n_init=20, random_state=seed+b)
        lbl = km.fit_predict(X_scaled[idx])
        subs.append((idx, lbl))
    aris=[]
    for i in range(len(subs)):
        for j in range(i+1, len(subs)):
            idx_i, lbl_i = subs[i]
            idx_j, lbl_j = subs[j]
            inter, ai, aj = np.intersect1d(idx_i, idx_j, return_indices=True)
            if len(inter) > 10:
                aris.append(adjusted_rand_score(lbl_i[ai], lbl_j[aj]))
    return float(np.mean(aris)) if aris else float("nan")

# Çalıştır
if 'X_scaled' in globals():
    _ = kmeans_grid_eval(X_scaled, k_grid=(3,4,5,6), repeats=5)
    if 'best_k' in globals() and best_k is not None:
        stability = kmeans_bootstrap_stability(X_scaled, best_k, B=20, frac=0.8)
        with open(os.path.join(OUTDIR, "kmeans_stability.txt"), "w") as f:
            f.write(f"best_k={best_k}, bootstrap_ARI_mean={stability:.3f}\n")
        print(f"[KMeans] Stability (ARI) ~ {stability:.3f} kaydedildi")

def dbscan_param_sweep(gdf, eps_list=(1.0,1.5,2.0,2.5,3.0), min_samples=3):
    if gdf.empty: return pd.DataFrame()
    out=[]
    coords = np.radians(gdf[["Latitude (generated)","Longitude (generated)"]].to_numpy())
    for eps_km in eps_list:
        db = DBSCAN(eps=eps_km/6371.0088, min_samples=min_samples, metric="haversine")
        labels = db.fit_predict(coords)
        n_clusters = len(set(l for l in labels if l != -1))
        noise_rate = float(np.mean(labels == -1))
        out.append({"eps_km": eps_km, "clusters": n_clusters, "noise_rate": noise_rate})
    return pd.DataFrame(out)

if 'gap_geo' in globals() and isinstance(gap_geo, pd.DataFrame) and not gap_geo.empty:
    sweep = dbscan_param_sweep(gap_geo, eps_list=(1.0,1.5,2.0,2.5,3.0), min_samples=3)
    if not sweep.empty:
        sweep.to_csv(os.path.join(OUTDIR, "dbscan_param_sweep.csv"), index=False)
        print("[DBSCAN] dbscan_param_sweep.csv kaydedildi")

print("[Eval] ek değerlendirme tamam.")

# [8] Harita (Folium)

try:
    import folium
    from IPython.display import display, IFrame
    from branca.colormap import linear

    html_path = os.path.join(OUTDIR, "nh_map.html")
    if not nh.empty and {'Latitude (generated)','Longitude (generated)'}.issubset(nh.columns):
        base = nh.dropna(subset=["Latitude (generated)","Longitude (generated)"]).copy()
        if len(base) > 3000: base = base.head(3000)

        m = folium.Map(location=[40.7128, -74.0060], zoom_start=10, tiles="cartodbpositron")

        # Güvenli colormap
        colormap = None
        if "need_score" in base.columns:
            s = pd.to_numeric(base["need_score"], errors="coerce")
            s = s[np.isfinite(s)]
            if len(s) >= 2:
                vmin, vmax = float(s.min()), float(s.max())
                if vmin < vmax:
                    colormap = linear.YlOrRd_09.scale(vmin, vmax)
                    colormap.caption = "Need Score"
                    colormap.add_to(m)

        fg_need = folium.FeatureGroup(name="Need Score", show=True)
        for _, row in base.iterrows():
            need_val = pd.to_numeric(row.get('need_score', np.nan), errors="coerce")
            color = "#3388ff" if (pd.isna(need_val) or colormap is None) else colormap(need_val)
            folium.CircleMarker(
                [row["Latitude (generated)"], row["Longitude (generated)"]],
                radius=4, color=color, fill=True, fill_color=color, fill_opacity=0.85,
                popup=(f"NTA: {row.get('NTA.Name','?')}<br>"
                       f"Need: {need_val if pd.notna(need_val) else 'NA'} | "
                       f"Service: {pd.to_numeric(row.get('service_score', np.nan), errors='coerce'):.3f}")
            ).add_to(fg_need)
        fg_need.add_to(m)

        # KMeans layer
        if "KMEANS_CLUSTER" in base.columns:
            fg_km = folium.FeatureGroup(name="KMeans Clusters", show=False)
            palette = ["#1f77b4","#ff7f0e","#2ca02c","#d62728","#9467bd",
                       "#8c564b","#e377c2","#7f7f7f","#bcbd22","#17becf"]
            for _, row in base.iterrows():
                cid = int(row.get("KMEANS_CLUSTER",-1))
                col = palette[cid % len(palette)] if cid >= 0 else "#666666"
                folium.CircleMarker(
                    [row["Latitude (generated)"], row["Longitude (generated)"]],
                    radius=4, color=col, fill=True, fill_color=col, fill_opacity=0.85,
                    popup=f"Cluster {cid} | NTA: {row.get('NTA.Name','?')}"
                ).add_to(fg_km)
            fg_km.add_to(m)

        # DBSCAN layer
        if "DBSCAN_CLUSTER" in nh.columns and nh["DBSCAN_CLUSTER"].notna().any():
            fg_db = folium.FeatureGroup(name="DBSCAN Hotspots", show=False)
            db_points = nh.dropna(subset=["Latitude (generated)","Longitude (generated)","DBSCAN_CLUSTER"])
            for _, row in db_points.iterrows():
                cid = int(row.get("DBSCAN_CLUSTER",-1))
                col = "#d62728" if cid >= 0 else "#999999"
                folium.CircleMarker(
                    [row["Latitude (generated)"], row["Longitude (generated)"]],
                    radius=6, color=col, fill=True, fill_color=col, fill_opacity=0.6,
                    popup=f"Hotspot {cid} | PRIORITY: {pd.to_numeric(row.get('PRIORITY_SCORE', np.nan), errors='coerce'):.3f}"
                ).add_to(fg_db)
            fg_db.add_to(m)

        folium.LayerControl().add_to(m)
        m.save(html_path)
        print(f"[MAP] Harita kaydedildi: {html_path}")
        try:
            display(m); display(IFrame(html_path, width="100%", height=650))
        except Exception:
            pass
    else:
        print("[MAP] NH lat/lon yok ya da NH boş; harita atlandı.")
except Exception as e:
    print("[MAP] Folium hatası:", e)

!pip install hdbscan

try:
    import hdbscan

    if not nh.empty and 'X_scaled' in globals():
        hdb = hdbscan.HDBSCAN(
            min_cluster_size=12,
            min_samples=5,
            metric="euclidean",
            prediction_data=True
        ).fit(X_scaled)

        nh["HDBSCAN_CLUSTER"] = hdb.labels_
        nh["HDBSCAN_CONF"] = hdb.probabilities_ if hasattr(hdb, "probabilities_") else np.nan

        # cluster_persistence_ küme-düzeyi → etiketlere map
        cluster_ids = [cid for cid in np.unique(hdb.labels_) if cid != -1]
        if hasattr(hdb, "cluster_persistence_") and len(hdb.cluster_persistence_) == len(cluster_ids):
            pers_map = {cid: float(pers) for cid, pers in zip(cluster_ids, hdb.cluster_persistence_)}
            nh["HDBSCAN_STABILITY"] = nh["HDBSCAN_CLUSTER"].map(pers_map).astype(float)
        else:
            nh["HDBSCAN_STABILITY"] = np.nan

        nh.to_csv(os.path.join(OUTDIR, "nh_with_hdbscan.csv"), index=False)
        print("[HDBSCAN] nh_with_hdbscan.csv yazıldı.")
    else:
        print("[HDBSCAN] nh boş veya X_scaled yok; atlandı.")
except Exception as e:
    print("[HDBSCAN] hata:", e)

# [10] Final rapor + model seçimi (KMeans vs HDBSCAN vs DBSCAN)
import json

# 1) Aday modellerden hangileri mevcut?
has_kmeans   = "KMEANS_CLUSTER"   in nh.columns
has_hdbscan  = "HDBSCAN_CLUSTER"  in nh.columns
has_dbscan   = "DBSCAN_CLUSTER"   in nh.columns

# 2) Basit seçim kuralları
# - HDBSCAN: en az 2 küme, ortalama istikrar >=0.50 ve gürültü oranı <0.60 ise tercih
# - Yoksa KMeans: silhouette >=0.45 ise tercih
# - Yoksa DBSCAN: en az 1 küme varsa tercih
# - Aksi halde KMeans (mevcutsa) ya da "none"

def choose_best():
    choice = {"model": "none", "reason": "", "metrics": {}}

    # HDBSCAN metrikleri (varsa)
    if has_hdbscan:
        n_hdb_clusters = int(nh.loc[nh["HDBSCAN_CLUSTER"] >= 0, "HDBSCAN_CLUSTER"].nunique())
        noise_rate     = float((nh["HDBSCAN_CLUSTER"] == -1).mean())
        mean_stab      = float(nh["HDBSCAN_STABILITY"].dropna().mean()) if "HDBSCAN_STABILITY" in nh.columns else float("nan")
        choice["metrics"].update(hdbscan_clusters=n_hdb_clusters, hdbscan_noise=noise_rate, hdbscan_mean_stability=mean_stab)

    # KMeans metrikleri (varsa)
    if has_kmeans:
        # best_k / best_score varsa kullan; yoksa silhouette'ı hızlıca hesapla
        km_sil = None
        if "best_score" in globals() and isinstance(best_score, (int,float)):
            km_sil = float(best_score)
        else:
            try:
                km_sil = float(silhouette_score(X_scaled, nh["KMEANS_CLUSTER"])) if 'X_scaled' in globals() else None
            except Exception:
                km_sil = None
        choice["metrics"].update(kmeans_silhouette=km_sil, kmeans_k=(int(best_k) if "best_k" in globals() and best_k is not None else None))

    # DBSCAN metrikleri (varsa)
    if has_dbscan:
        n_db_clusters = int(nh.loc[nh["DBSCAN_CLUSTER"] >= 0, "DBSCAN_CLUSTER"].nunique())
        choice["metrics"].update(dbscan_clusters=n_db_clusters)

    # ---- Kural tabanlı seçim ----
    # Öncelik HDBSCAN
    if has_hdbscan:
        n_hdb_clusters = choice["metrics"].get("hdbscan_clusters", 0)
        mean_stab      = choice["metrics"].get("hdbscan_mean_stability", float("nan"))
        noise_rate     = choice["metrics"].get("hdbscan_noise", 1.0)
        if n_hdb_clusters >= 2 and (not np.isnan(mean_stab) and mean_stab >= 0.50) and noise_rate < 0.60:
            choice.update(model="HDBSCAN", reason=f"HDBSCAN istikrarlı (clusters={n_hdb_clusters}, mean_stab={mean_stab:.2f}, noise={noise_rate:.2f})")

    # Sonra KMeans
    if choice["model"] == "none" and has_kmeans:
        km_sil = choice["metrics"].get("kmeans_silhouette", None)
        if km_sil is not None and km_sil >= 0.45:
            choice.update(model="KMEANS", reason=f"KMeans iyi ayrışım (silhouette={km_sil:.2f})")

    # Son olarak DBSCAN
    if choice["model"] == "none" and has_dbscan:
        n_db_clusters = choice["metrics"].get("dbscan_clusters", 0)
        if n_db_clusters >= 1:
            choice.update(model="DBSCAN", reason=f"DBSCAN en az bir anlamlı küme (clusters={n_db_clusters})")

    # Fallback
    if choice["model"] == "none" and has_kmeans:
        choice.update(model="KMEANS", reason="Diğer koşullar zayıf; KMeans fallback")

    return choice

selection = choose_best()
print("[Model seçimi]", selection)

# 3) Final rapor — temel kolonları topla
final_cols = [
    # kimlik / geo
    "NTA","NTA.Name","BORO",
    "Latitude (generated)","Longitude (generated)",
    # skorlar
    "need_score","service_score","PRIORITY_SCORE",
    "efap_sites","total_days",
    # küme etiketleri
    "KMEANS_CLUSTER","DBSCAN_CLUSTER","HDBSCAN_CLUSTER","HDBSCAN_CONF","HDBSCAN_STABILITY"
]
final_cols = [c for c in final_cols if c in nh.columns]

# [ADD 5.5] Bağış/STK öncelik skoru + (opsiyonel) STK önerileri
_ = _compute_donation_priority(nh, stk_path=STK_CSV, outdir=OUTDIR)


final = nh[final_cols].copy()

# 4) Seçilen modeli 'SELECTED_CLUSTER' sütununa yaz
sel_model = selection["model"]
if sel_model == "KMEANS" and "KMEANS_CLUSTER" in final.columns:
    final["SELECTED_MODEL"]   = "KMEANS"
    final["SELECTED_CLUSTER"] = final["KMEANS_CLUSTER"]
elif sel_model == "HDBSCAN" and "HDBSCAN_CLUSTER" in final.columns:
    final["SELECTED_MODEL"]   = "HDBSCAN"
    final["SELECTED_CLUSTER"] = final["HDBSCAN_CLUSTER"]
elif sel_model == "DBSCAN" and "DBSCAN_CLUSTER" in final.columns:
    final["SELECTED_MODEL"]   = "DBSCAN"
    final["SELECTED_CLUSTER"] = final["DBSCAN_CLUSTER"]
else:
    final["SELECTED_MODEL"]   = "NONE"
    final["SELECTED_CLUSTER"] = np.nan

# 5) Kaydet
final_path = os.path.join(OUTDIR, "final_report.csv")
final.to_csv(final_path, index=False)

with open(os.path.join(OUTDIR, "model_selection.json"), "w") as f:
    json.dump(selection, f, indent=2)

print(f"[FINAL] final_report -> {final_path}")
print(f"[FINAL] model_selection -> {os.path.join(OUTDIR, 'model_selection.json')}")

OUTDIR = "outputs"
os.makedirs(OUTDIR, exist_ok=True)

final_path = os.path.join(OUTDIR, "final_report.csv")
final.to_csv(final_path, index=False)

with open(os.path.join(OUTDIR, "model_selection.json"), "w") as f:
    json.dump(selection, f, indent=2)

print("[FINAL] final_report ->", final_path)

# (isteğe bağlı) indir
try:
    from google.colab import files
    files.download(final_path)
except Exception:
    pass

def merge_flw_with_efap():
    import pandas as pd, numpy as np, os

    # Dosya yolları
    flw_path = "flw_cleaned.csv"
    efap_path = "outputs/nh_cleaned.csv"

    if not os.path.exists(flw_path):
        print(f"[HATA] FLW dosyası bulunamadı: {flw_path}")
        return
    if not os.path.exists(efap_path):
        print(f"[HATA] EFAP/NH dosyası bulunamadı: {efap_path}")
        return

    # FLW verisini oku
    flw = pd.read_csv(flw_path)

    # ABD satırlarını filtrele
    mask_us = pd.Series(False, index=flw.index)
    for c in flw.columns:
        if flw[c].dtype == "O":
            mask_us |= flw[c].astype(str).str.contains("United States|America", case=False, na=False)
    flw_us = flw[mask_us].copy()

    # Yıl ve loss% işle
    flw_us["year"] = pd.to_numeric(flw_us.get("year", np.nan), errors="coerce").astype("Int64")
    flw_us["loss_percentage"] = pd.to_numeric(flw_us.get("loss_percentage", np.nan), errors="coerce")

    # Yıla göre median loss%
    flw_year = (flw_us
                .dropna(subset=["loss_percentage"])
                .groupby("year", dropna=True)["loss_percentage"]
                .median()
                .rename("us_loss_pct")
                .reset_index())

    # En son yıl değeri (fallback)
    latest_us_loss = flw_year["us_loss_pct"].iloc[-1] if not flw_year.empty else np.nan

    # EFAP/NH oku
    nh = pd.read_csv(efap_path)

    # Yıl var mı?
    year_col = None
    for cand in ["year", "YEAR", "Year"]:
        if cand in nh.columns:
            year_col = cand
            break

    # Birleştir
    if year_col is not None and nh[year_col].notna().any():
        nh[year_col] = pd.to_numeric(nh[year_col], errors="coerce").astype("Int64")
        model_df = nh.merge(flw_year, left_on=year_col, right_on="year", how="left")
        model_df.drop(columns=["year"], inplace=True, errors="ignore")
        if "us_loss_pct" in model_df.columns and not np.isnan(latest_us_loss):
            model_df["us_loss_pct"] = model_df["us_loss_pct"].fillna(latest_us_loss)
    else:
        model_df = nh.copy()
        model_df["us_loss_pct"] = latest_us_loss

    # Kaydet
    os.makedirs("outputs", exist_ok=True)
    out_path = "outputs/model_input_efap.csv"
    model_df.to_csv(out_path, index=False)
    print(f"[OK] Model girdi dosyası kaydedildi → {out_path}")
    print(model_df.head())

# EFAP çalıştıktan sonra FLW ile birleştir
merge_flw_with_efap()
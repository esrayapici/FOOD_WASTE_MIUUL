# -*- coding: utf-8 -*-
"""Pipeline_proje_food_bridge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IkB6eDuyzT2rSJVmMl1-0eUpyAhCu3Zd
"""

# ====== CELL 1: Ortamı Hazırla (Colab) ======
# Bu hücreyi bir kerede çalıştırın.
!pip -q install --upgrade pip
!pip -q install lightgbm xgboost lifetimes sentence-transformers \
                 osmnx geopandas shapely pyproj rtree \
                 rapidfuzz geopy hdbscan shap tqdm mlxtend joblib

# (Not): osmnx/geopandas kurulumu biraz zaman alabilir.

# Commented out IPython magic to ensure Python compatibility.
# ====== CELL 2: (Opsiyonel) Google Drive'ı Bağla ======
# Eğer dosyalarınız Drive'daysa açın, yoksa atlayın.
# from google.colab import drive
# drive.mount('/content/drive')
# PROJECT_DIR = "/content/drive/MyDrive/food_waste_project"  # klasörünüz
# !mkdir -p "$PROJECT_DIR"
# %cd "$PROJECT_DIR"

# Eğer dosyaları doğrudan bu çalışma alanına yükleyecekseniz:
# %cd /content

# ==== CELL 3A: Dosya kontrolleri ====
import os
from pathlib import Path

print("/content içeriği (ilk 20):", os.listdir("/content")[:20])

required_files = [
    "/content/pipeline.py",
    "/content/rfm_anaylsis.py",
    "/content/efapsonn.py",
    "/content/cltv.py",
    "/content/food_waste_end_to_end_pipeline.py",
]
optional_files = ["/content/K3.py", "/content/K4.py"]
required_csv = [
    "/content/pos.csv",
    "/content/Neighborhood_Prioritization_Map_2024.csv",
]

for f in required_files:
    if not Path(f).exists():
        print("Eksik .py:", f)
for f in optional_files:
    if not Path(f).exists():
        print("Opsiyonel yok (sorun değil):", f)
for f in required_csv:
    if not Path(f).exists():
        print("Eksik CSV:", f)

# ==== CELL 3B: cltv_safe.py (CLTV fallback) yaz ====
cltv_safe_code = r'''
import pandas as pd
import numpy as np

def _cltv_lite_from_df(df: pd.DataFrame,
                       customer_id_col="Customer_ID",
                       timestamp_col="Timestamp",
                       total_price_col="Total_Price"):
    dfc = df.copy()
    if not pd.api.types.is_datetime64_any_dtype(dfc[timestamp_col]):
        dfc[timestamp_col] = pd.to_datetime(dfc[timestamp_col])

    cust = dfc.groupby(customer_id_col).agg(
        order_count=(timestamp_col, "nunique"),
        avg_value=(total_price_col, "mean"),
        first_order=(timestamp_col, "min"),
        last_order=(timestamp_col, "max"),
    ).reset_index().rename(columns={customer_id_col: "customer_id"})

    analysis_date = dfc[timestamp_col].max() + pd.Timedelta(days=1)
    cust["T_weekly"] = ((analysis_date - cust["first_order"]).dt.days) / 7.0
    cust["T_weekly"] = cust["T_weekly"].replace(0, 1e-6)

    total_orders = cust["order_count"].sum()
    total_weeks = cust["T_weekly"].sum()
    lam_global = (total_orders / total_weeks) if total_weeks > 0 else 0.0

    cust["exp_sales_3_month"] = lam_global * 12.0
    cust["exp_sales_6_month"] = lam_global * 24.0

    cust["exp_average_value"] = pd.to_numeric(cust["avg_value"], errors="coerce")
    if cust["exp_average_value"].isna().all():
        cust["exp_average_value"] = 0.0
    else:
        cust["exp_average_value"] = cust["exp_average_value"].fillna(cust["exp_average_value"].median())

    cust["cltv_3m"] = cust["exp_sales_3_month"] * cust["exp_average_value"]
    cust["cltv_6m"] = cust["exp_sales_6_month"] * cust["exp_average_value"]
    cust["cltv"]     = cust["cltv_6m"]

    try:
        cust["cltv_segment"] = pd.qcut(
            cust["cltv"].rank(method="first"), 4, labels=["D","C","B","A"]
        )
    except Exception:
        cust["cltv_segment"] = "C"

    out_cols = [
        "customer_id","cltv","cltv_3m","cltv_6m",
        "exp_sales_3_month","exp_sales_6_month",
        "exp_average_value","cltv_segment"
    ]
    return cust[out_cols]


def create_cltv_or_fallback(df: pd.DataFrame,
                            customer_id_col="Customer_ID",
                            timestamp_col="Timestamp",
                            total_price_col="Total_Price"):
    try:
        from cltv import create_cltv_df as _orig
        res = _orig(df.copy(),
                    customer_id_col=customer_id_col,
                    timestamp_col=timestamp_col,
                    total_price_col=total_price_col)
        if res is None or (isinstance(res, pd.DataFrame) and res.empty):
            print("[CLTV] Orijinal model boş → CLTV-lite devrede.")
            return _cltv_lite_from_df(df,
                                      customer_id_col=customer_id_col,
                                      timestamp_col=timestamp_col,
                                      total_price_col=total_price_col)
        print("[CLTV] Orijinal model sonuç verdi.")
        return res
    except Exception as e:
        print("[CLTV] Orijinal modelde hata:", e)
        print("[CLTV] CLTV-lite devreye alınıyor.")
        return _cltv_lite_from_df(df,
                                  customer_id_col=customer_id_col,
                                  timestamp_col=timestamp_col,
                                  total_price_col=total_price_col)
'''
with open("/content/cltv_safe.py", "w", encoding="utf-8") as f:
    f.write(cltv_safe_code)

print("↳ Yazıldı: /content/cltv_safe.py")

# ==== CELL 3C: food_waste_end_to_end_pipeline.py patch ====
from pathlib import Path
import re

orc_path = Path("/content/food_waste_end_to_end_pipeline.py")
assert orc_path.exists(), "food_waste_end_to_end_pipeline.py bulunamadı"
src = orc_path.read_text(encoding="utf-8")

# 1) CLTV import & çağrılarını değiştir
src = src.replace(
    "from cltv import create_cltv_df",
    "from cltv_safe import create_cltv_or_fallback"
)
src = src.replace(
    "cltv_df = create_cltv_df(df, customer_id_col=customer_id_col, timestamp_col=timestamp_col, total_price_col=total_price_col)",
    "cltv_df = create_cltv_or_fallback(df, customer_id_col=customer_id_col, timestamp_col=timestamp_col, total_price_col=total_price_col)"
)

# 2) is_holiday hotfix: aggregate sonrası tatil sütunu ekle
needle = "daily = aggregate_daily_item_demand(df_pos)"
patch  = """daily = aggregate_daily_item_demand(df_pos)
    # ---- is_holiday hotfix (pipeline.build_preprocessor bekliyor) ----
    try:
        dts = pd.to_datetime(daily['date'])
        from pandas.tseries.holiday import USFederalHolidayCalendar
        cal = USFederalHolidayCalendar()
        hol = cal.holidays(start=str(dts.min().date()), end=str(dts.max().date()))
        daily['is_holiday'] = dts.isin(hol).astype(int)
    except Exception:
        daily['is_holiday'] = 0
"""
if needle in src and "is_holiday" not in src:
    src = src.replace(needle, patch, 1)

orc_path.write_text(src, encoding="utf-8")
print("↳ Orchestrator patchlendi (CLTV fallback + is_holiday).")

# ==== CELL 3.5: Donation için gerekli dosya kontrolü ====
from pathlib import Path

need = [
    "/content/efapsonn.py",                                   # donation hesaplama kodu
    "/content/Neighborhood_Prioritization_Map_2024.csv",      # NH verisi
    "/content/EFAP_pdf_11_4_24.csv",                          # (varsa) EFAP verisi, yoksa sorun değil
    "/content/STK.csv",                                       # (opsiyonel) STK listesi
]
for p in need:
    print(("OK  " if Path(p).exists() else "MISS"), p)

# ==== CELL 4: Parametreler ====
POS_CSV = "/content/pos.csv"
NH_CSV = "/content/Neighborhood_Prioritization_Map_2024.csv"
OUTPUT_DIR = "/content/outputs"

CUSTOMER_COL = "Customer_ID"
TIMESTAMP_COL = "Timestamp"
TOTALPRICE_COL = "Total_Price"

RUN_DEMAND = True
RUN_RFM = True
RUN_CLTV = True
RUN_NLP = False
RUN_GEO = False
RUN_DONATION = False

HORIZON = 7
SWEEP_CANDIDATES = 12

# ==== CELL 4.5: EFAPSONN PATCH (NaN güvenliği ve min/max düzeltmeleri) ====
from pathlib import Path
import re, py_compile
import pandas as pd, numpy as np

p = Path("/content/efapsonn.py")
src = p.read_text(encoding="utf-8")

# _zminmax düzeltme
src = re.sub(
    r"def _zminmax\(s: pd\.Series\):[\s\S]*?return \(s - vmin\) / \(vmax - vmin\)",
    """def _zminmax(s: pd.Series):
    import pandas as _pd, numpy as _np
    if not hasattr(s, 'index'):
        s = _pd.Series(s)
    s = _pd.to_numeric(s, errors="coerce")
    vmin, vmax = s.min(skipna=True), s.max(skipna=True)
    if _pd.isna(vmin) or _pd.isna(vmax) or vmin == vmax:
        return _pd.Series(_np.nan, index=s.index)
    return (s - vmin) / (vmax - vmin)""",
    src,
    flags=re.MULTILINE
)

# _zminmax_55 düzeltme
src = re.sub(
    r"def _zminmax_55\(s: pd\.Series\):[\s\S]*?return \(s - vmin\) / \(vmax - vmin\)",
    """def _zminmax_55(s: pd.Series):
    import pandas as _pd, numpy as _np
    if not hasattr(s, 'index'):
        s = _pd.Series(s)
    s = _pd.to_numeric(s, errors="coerce")
    vmin, vmax = s.min(skipna=True), s.max(skipna=True)
    if _pd.isna(vmin) or _pd.isna(vmax) or vmin == vmax:
        return _pd.Series(_np.nan, index=s.index)
    return (s - vmin) / (vmax - vmin)""",
    src,
    flags=re.MULTILINE
)

# Donation skor kısmı NaN güvenliği
src = src.replace(
    'need = df["PRIORITY_SCORE"] if "PRIORITY_SCORE" in df.columns else df.get("need_score", np.nan)',
    'need = df["PRIORITY_SCORE"] if "PRIORITY_SCORE" in df.columns else df.get("need_score", pd.Series(np.nan, index=df.index))'
)
src = src.replace(
    'svc  = df.get("service_score", np.nan)',
    'svc  = df.get("service_score", pd.Series(np.nan, index=df.index))'
)
src = src.replace(
    'flw  = df.get("us_loss_pct", np.nan)',
    'flw  = df.get("us_loss_pct", pd.Series(np.nan, index=df.index))'
)

# Dosyayı geri yaz
p.write_text(src, encoding="utf-8")
print("✔ efapsonn.py patch uygulandı.")

# Sözdizimi kontrol
try:
    py_compile.compile(str(p), doraise=True)
    print(" Sözdizimi OK")
except py_compile.PyCompileError as e:
    print(" SyntaxError:", e.msg)

# ==== TEMP FIX: POS verisini pipeline'ın beklediği isimle kopyala ====
!cp /content/pos.csv /content/simulated_pos_data_with_seasonal_trends.csv
print("POS verisi kopyalandı:", "/content/simulated_pos_data_with_seasonal_trends.csv")

# ==== CELL 4.5: Sadece donation adımı çalıştır (debug) ====
import shlex, subprocess, sys, os

args = [
    sys.executable, "/content/food_waste_end_to_end_pipeline.py",
    "--pos-csv", "/content/pos.csv",    # donation bu dosyayı kullanmıyor ama parser istiyor
    "--output-dir", "/content/outputs",
    "--nh-csv", "/content/Neighborhood_Prioritization_Map_2024.csv",
    "--run-donation",
]

stk_path = "/content/STK.csv"
if os.path.exists(stk_path):
    args += ["--stk-csv", stk_path]

print("Cmd:\n", " ".join(shlex.quote(a) for a in args))
proc = subprocess.run(args, text=True, capture_output=True)
print("\n--- STDOUT ---\n", proc.stdout)
print("\n--- STDERR ---\n", proc.stderr)
print("\nExit code:", proc.returncode)

# ==== PATCH: CLTV None kontrolünü düzelt ====
from pathlib import Path

p = Path("/content/food_waste_end_to_end_pipeline.py")
src = p.read_text(encoding="utf-8")

src = src.replace(
    "if create_cltv_df is None:",
    "if create_cltv_or_fallback is None:"
)

# Güvenlik: import satırının da doğru olduğundan emin olalım
src = src.replace(
    "from cltv import create_cltv_df",
    "from cltv_safe import create_cltv_or_fallback"
)

p.write_text(src, encoding="utf-8")
print(" CLTV kontrolü ve import satırı düzeltildi.")

# ==== RUN (DEBUG) ====
import shlex, subprocess, sys
args = [
    sys.executable, "/content/food_waste_end_to_end_pipeline.py",
    "--pos-csv", "/content/pos.csv",
    "--output-dir", "/content/outputs",
    "--nh-csv", "/content/Neighborhood_Prioritization_Map_2024.csv",
    "--customer-col", "Customer_ID",
    "--timestamp-col", "Timestamp",
    "--totalprice-col", "Total_Price",
    "--run-demand", "--run-rfm", "--run-cltv",
    "--horizon", "7",
    "--sweep-candidates", "12",
]
print("Cmd:\n", " ".join(shlex.quote(a) for a in args))
proc = subprocess.run(args, text=True, capture_output=True)
print("\n--- STDOUT ---\n", proc.stdout)
print("\n--- STDERR ---\n", proc.stderr)
print("\nExit code:", proc.returncode)

# ==== CELL 5.5: Donation çıktısını önizle ====
import os
import pandas as pd

out = "/content/outputs/donation/nh_with_donation_priority.csv"
if os.path.exists(out):
    print("OK →", out)
    display(pd.read_csv(out).head())
else:
    print("Bulunamadı:", out, " (Cell 4.5 çıktısındaki hataya bak)")

# ==== CELL 6: Çıktı kontrol ====
import os, pandas as pd
paths = [
    f"{OUTPUT_DIR}/demand/forecast_next_{HORIZON}d.csv",
    f"{OUTPUT_DIR}/crm_rfm/rfm_segments.csv",
    f"{OUTPUT_DIR}/crm_cltv/cltv_segments.csv",
    f"{OUTPUT_DIR}/donation/nh_with_donation_priority.csv",
]
for p in paths:
    if os.path.exists(p):
        print("\nOK →", p)
        try:
            display(pd.read_csv(p).head())
        except Exception as e:
            print("Önizleme okunamadı:", e)
    else:
        print("\n— Bulunamadı:", p)

'''# ==== SAFE ZIP: /content altındaki TÜM dosyaları (alt klasörler dahil) indir ====
import os, zipfile

BASE_DIR = "/content"
ZIP_PATH = os.path.join(BASE_DIR, "all_content_files.zip")
EXCLUDE_DIRS = {
    os.path.join(BASE_DIR, "download_all"),
    os.path.join(BASE_DIR, "download_all_"),# önceki geçici klasörler
}
EXCLUDE_FILES = {
    ZIP_PATH,  # oluşturduğumuz zip'in kendisi
}

# Eski zip varsa sil
if os.path.exists(ZIP_PATH):
    os.remove(ZIP_PATH)

with zipfile.ZipFile(ZIP_PATH, "w", compression=zipfile.ZIP_DEFLATED) as z:
    for root, dirs, files in os.walk(BASE_DIR):
        # dışlanacak klasörleri gezme
        dirs[:] = [d for d in dirs if os.path.join(root, d) not in EXCLUDE_DIRS]
        for fn in files:
            full = os.path.join(root, fn)
            # zip'in kendisini ekleme
            if os.path.abspath(full) in EXCLUDE_FILES:
                continue
            # istersen burada belirli uzantıları filtreleyebilirsin
            arc = os.path.relpath(full, BASE_DIR)
            z.write(full, arc)

print(" ZIP hazır:", ZIP_PATH)

from google.colab import files
files.download(ZIP_PATH)
'''

"""# Gradio App"""

!pip -q install gradio pandas

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app_gradio.py
# import gradio as gr
# import pandas as pd
# import subprocess, sys, os, tempfile
# 
# def write_uploaded(upl, dest_path):
#     """
#     Gradio File bileşeninden gelen girdiyi (filepath/bytes/dict) diske yazar.
#     """
#     if upl is None:
#         return None
#     # upl bir yol ise:
#     if isinstance(upl, (str, os.PathLike)):
#         with open(upl, "rb") as src, open(dest_path, "wb") as dst:
#             dst.write(src.read())
#         return dest_path
#     # upl dict ise (bazı sürümlerde {'name','orig_name','size','path'} gelebilir)
#     if isinstance(upl, dict) and "path" in upl:
#         with open(upl["path"], "rb") as src, open(dest_path, "wb") as dst:
#             dst.write(src.read())
#         return dest_path
#     # upl bytes ise:
#     if isinstance(upl, (bytes, bytearray)):
#         with open(dest_path, "wb") as dst:
#             dst.write(upl)
#         return dest_path
#     # file-like ise:
#     if hasattr(upl, "read"):
#         with open(dest_path, "wb") as dst:
#             dst.write(upl.read())
#         return dest_path
#     raise ValueError("Desteklenmeyen dosya tipi:", type(upl))
# 
# def run_pipeline(pos_file, nh_file, stk_file,
#                  horizon, sweep, run_demand, run_rfm, run_cltv, run_donation,
#                  customer_col, timestamp_col, totalprice_col):
#     if pos_file is None:
#         return "POS dosyası gerekli.", "", None, None, None, None, [], []
# 
#     workdir = tempfile.mkdtemp(prefix="fw_pipeline_")
#     outdir  = os.path.join(workdir, "outputs")
#     os.makedirs(outdir, exist_ok=True)
# 
#     pos_path = os.path.join(workdir, "pos.csv")
#     write_uploaded(pos_file, pos_path)
# 
#     nh_path = None
#     if nh_file is not None:
#         nh_path = os.path.join(workdir, "Neighborhood_Prioritization_Map_2024.csv")
#         write_uploaded(nh_file, nh_path)
# 
#     stk_path = None
#     if stk_file is not None:
#         stk_path = os.path.join(workdir, "STK.csv")
#         write_uploaded(stk_file, stk_path)
# 
#     cmd = [
#         sys.executable, "food_waste_end_to_end_pipeline.py",
#         "--pos-csv", pos_path,
#         "--output-dir", outdir,
#         "--customer-col", customer_col,
#         "--timestamp-col", timestamp_col,
#         "--totalprice-col", totalprice_col,
#         "--horizon", str(int(horizon)),
#         "--sweep-candidates", str(int(sweep)),
#     ]
#     if run_demand:   cmd.append("--run-demand")
#     if run_rfm:      cmd.append("--run-rfm")
#     if run_cltv:     cmd.append("--run-cltv")
#     if run_donation and nh_path is not None:
#         cmd += ["--nh-csv", nh_path, "--run-donation"]
#         if stk_path is not None:
#             cmd += ["--stk-csv", stk_path]
# 
#     proc = subprocess.run(cmd, text=True, capture_output=True, cwd=".")
# 
#     demand_p = os.path.join(outdir, f"demand/forecast_next_{int(horizon)}d.csv")
#     rfm_p    = os.path.join(outdir, "crm_rfm/rfm_segments.csv")
#     cltv_p   = os.path.join(outdir, "crm_cltv/cltv_segments.csv")
#     don_p    = os.path.join(outdir, "donation/nh_with_donation_priority.csv")
# 
#     def safe_read(p):
#         try:
#             if os.path.exists(p):
#                 return pd.read_csv(p)
#         except Exception:
#             pass
#         return None
# 
#     df_demand = safe_read(demand_p)
#     df_rfm    = safe_read(rfm_p)
#     df_cltv   = safe_read(cltv_p)
#     df_don    = safe_read(don_p)
# 
#     download_files, labels = [], []
#     for p, lab in [(demand_p,"demand"), (rfm_p,"rfm"), (cltv_p,"cltv"), (don_p,"donation")]:
#         if os.path.exists(p):
#             download_files.append(p)
#             labels.append(lab)
# 
#     return proc.stdout, proc.stderr, df_demand, df_rfm, df_cltv, df_don, download_files, labels
# 
# with gr.Blocks(title="Food Bridge – Project Pipeline (Gradio)") as demo:
#     gr.Markdown("##  Food Waste – Project Pipeline (Gradio)")
#     with gr.Row():
#         with gr.Column():
#             # type="filepath" -> fonksiyonda path üzerinden okuyoruz
#             pos = gr.File(label="POS CSV", file_types=[".csv"], type="filepath")
#             nh  = gr.File(label="Neighborhood_Prioritization_Map_2024.csv (Donation)", file_types=[".csv"], type="filepath")
#             stk = gr.File(label="STK.csv (opsiyonel, Donation)", file_types=[".csv"], type="filepath")
#             horizon = gr.Number(label="Tahmin ufku (gün)", value=7, precision=0)
#             sweep   = gr.Number(label="Sweep candidates", value=12, precision=0)
#             run_demand   = gr.Checkbox(label="Talep Tahmini", value=True)
#             run_rfm      = gr.Checkbox(label="RFM", value=True)
#             run_cltv     = gr.Checkbox(label="CLTV", value=True)
#             run_donation = gr.Checkbox(label="Donation (bağış)", value=False)
#             customer_col   = gr.Textbox(label="Customer ID kolonu", value="Customer_ID")
#             timestamp_col  = gr.Textbox(label="Timestamp kolonu", value="Timestamp")
#             totalprice_col = gr.Textbox(label="Total Price kolonu", value="Total_Price")
#             run_btn = gr.Button("Çalıştır", variant="primary")
# 
#         with gr.Column():
#             stdout = gr.Textbox(label="STDOUT", lines=12)
#             stderr = gr.Textbox(label="STDERR", lines=12)
# 
#     with gr.Tab("Talep"):
#         df_demand = gr.Dataframe(label="forecast_next_*d.csv", wrap=True)
#     with gr.Tab("RFM"):
#         df_rfm = gr.Dataframe(label="rfm_segments.csv", wrap=True)
#     with gr.Tab("CLTV"):
#         df_cltv = gr.Dataframe(label="cltv_segments.csv", wrap=True)
#     with gr.Tab("Donation"):
#         df_don = gr.Dataframe(label="nh_with_donation_priority.csv", wrap=True)
#     with gr.Tab("İndir"):
#         files = gr.Files(label="Üretilen CSV'ler")
#         labels = gr.JSON(label="Dosya etiketleri")
# 
#     run_btn.click(
#         run_pipeline,
#         inputs=[pos, nh, stk, horizon, sweep, run_demand, run_rfm, run_cltv, run_donation,
#                 customer_col, timestamp_col, totalprice_col],
#         outputs=[stdout, stderr, df_demand, df_rfm, df_cltv, df_don, files, labels]
#     )
# 
# if __name__ == "__main__":
#     demo.launch(share=True)
#

!python app_gradio.py

